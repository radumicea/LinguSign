{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Romanian to Sign Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "import concurrent\n",
        "import cv2\n",
        "import os\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import re\n",
        "import requests\n",
        "import ssl\n",
        "import time\n",
        "\n",
        "from aiohttp import TCPConnector\n",
        "from bs4 import BeautifulSoup\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from natsort import os_sorted"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# USB webcam\n",
        "cam_idx = 0\n",
        "\n",
        "# Recording is done on Ubuntu\n",
        "api_pref = cv2.CAP_V4L2\n",
        "\n",
        "# Target frame resolution\n",
        "width = 640\n",
        "height = 480\n",
        "\n",
        "scraped_dataset_path = os.path.join('..', 'scraped', 'pesemne')\n",
        "output_path = os.path.join('..', 'scraped', 'pesemne_processed')\n",
        "landmarks_path = os.path.join('assets', 'landmarks')\n",
        "\n",
        "\n",
        "hands_base_options = python.BaseOptions(model_asset_path=os.path.join('assets', 'mp_models', 'hand_landmarker.task'))\n",
        "hands_options = vision.HandLandmarkerOptions(base_options=hands_base_options,\n",
        "                                       running_mode=mp.tasks.vision.RunningMode.VIDEO,\n",
        "                                       num_hands=2,\n",
        "                                       min_hand_detection_confidence=0.75,\n",
        "                                       min_hand_presence_confidence=0.75,\n",
        "                                       min_tracking_confidence=0.75)\n",
        "\n",
        "pose_base_options = python.BaseOptions(model_asset_path=os.path.join('assets', 'mp_models', 'pose_landmarker_full.task'))\n",
        "pose_options = vision.PoseLandmarkerOptions(base_options=pose_base_options,\n",
        "                                       running_mode=mp.tasks.vision.RunningMode.VIDEO,\n",
        "                                       min_pose_detection_confidence=0.5,\n",
        "                                       min_pose_presence_confidence=0.5,\n",
        "                                       min_tracking_confidence=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pose_landmarks(results):\n",
        "    pose = None\n",
        "\n",
        "    for pose_landmarks in results.pose_landmarks:\n",
        "        pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "        pose_landmarks_proto.landmark.extend(\n",
        "            [landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z, visibility=landmark.visibility) for landmark in pose_landmarks]\n",
        "            )\n",
        "        \n",
        "        pose = pose_landmarks_proto\n",
        "\n",
        "    return pose\n",
        "\n",
        "\n",
        "def get_hands_landmarks(results):\n",
        "    left = [None]\n",
        "    right = [None]\n",
        "\n",
        "    # handedness list and hand_landmarks list match by index\n",
        "    for handedness, hand_landmarks in zip(results.handedness, results.hand_landmarks):\n",
        "        handedness = handedness[0]\n",
        "\n",
        "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
        "        hand_landmarks_proto.landmark.extend(\n",
        "            [landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks]\n",
        "            )\n",
        "        \n",
        "        # Assume we are pointing the camera to a person with one left and one right hand.\n",
        "        # That means that the only correct predicted handednesses are { 'Left' AND 'Right' }\n",
        "\n",
        "        # If predicted handedness is left\n",
        "        if handedness.category_name == 'Left':\n",
        "            # If left was NOT predicted before\n",
        "            if left[0] is None:\n",
        "                # Just assign the landmarks and handedness probability\n",
        "                left = [hand_landmarks_proto, handedness.score]\n",
        "            # Else, if we have a collision\n",
        "            else:\n",
        "                # If current prediction is more accurate:\n",
        "                if handedness.score > left[1]:\n",
        "                    # That means that the previous left value is actually right\n",
        "                    right = left\n",
        "                    left = [hand_landmarks_proto, handedness.score]\n",
        "                # Else, if current prediction is less accurate\n",
        "                else:\n",
        "                    # That means that the current prediction is actually right\n",
        "                    right = [hand_landmarks_proto, handedness.score]\n",
        "        \n",
        "        # Same exact thing for right\n",
        "        if handedness.category_name == 'Right':\n",
        "            if right[0] is None:\n",
        "                right = [hand_landmarks_proto, handedness.score]\n",
        "            else:\n",
        "                if handedness.score > right[1]:\n",
        "                    left = right\n",
        "                    right = [hand_landmarks_proto, handedness.score]\n",
        "                else:\n",
        "                    left = [hand_landmarks_proto, handedness.score]\n",
        "\n",
        "    return left[0], right[0]\n",
        "\n",
        "\n",
        "def draw_results(frame, pose, left_hand, right_hand):    \n",
        "    mp.solutions.drawing_utils.draw_landmarks(\n",
        "        frame,\n",
        "        pose,\n",
        "        mp.solutions.holistic.POSE_CONNECTIONS,\n",
        "        landmark_drawing_spec=mp.solutions.drawing_styles\n",
        "            .get_default_pose_landmarks_style())\n",
        "    \n",
        "    mp.solutions.drawing_utils.draw_landmarks(\n",
        "        frame,\n",
        "        left_hand,\n",
        "        mp.solutions.holistic.HAND_CONNECTIONS,\n",
        "        landmark_drawing_spec=mp.solutions.drawing_styles\n",
        "            .get_default_hand_landmarks_style(),\n",
        "        connection_drawing_spec=mp.solutions.drawing_styles\n",
        "            .get_default_hand_connections_style())\n",
        "    \n",
        "    mp.solutions.drawing_utils.draw_landmarks(\n",
        "        frame,\n",
        "        right_hand,\n",
        "        mp.solutions.holistic.HAND_CONNECTIONS,\n",
        "        landmark_drawing_spec=mp.solutions.drawing_styles\n",
        "            .get_default_hand_landmarks_style(),\n",
        "        connection_drawing_spec=mp.solutions.drawing_styles\n",
        "            .get_default_hand_connections_style())\n",
        "    \n",
        "    \n",
        "def to_results_dict(pose, left_hand, right_hand):\n",
        "    pose = [\n",
        "        [[p.x, p.y, p.z] for p in r.landmark] if r is not None else [[0] * 3 for _ in range(33)]\n",
        "        for r in pose\n",
        "    ]\n",
        "\n",
        "    left_hand = [\n",
        "        [[h.x, h.y, h.z] for h in r.landmark] if r is not None else [[0] * 3 for _ in range(21)]\n",
        "        for r in left_hand\n",
        "    ]\n",
        "\n",
        "    right_hand = [\n",
        "        [[h.x, h.y, h.z] for h in r.landmark] if r is not None else [[0] * 3 for _ in range(21)]\n",
        "        for r in right_hand\n",
        "    ]\n",
        "    \n",
        "    return {\n",
        "        'pose': pose,\n",
        "        'left_hand': left_hand,\n",
        "        'right_hand': right_hand\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process Videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_video(video_path):\n",
        "    video_name = video_path.split(os.sep)[-1].split('.')[0]\n",
        "\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    fps = int(round(cap.get(cv2.CAP_PROP_FPS)))\n",
        "    w  = int(round(cap.get(cv2.CAP_PROP_FRAME_WIDTH)))\n",
        "    h = int(round(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
        "\n",
        "    out = cv2.VideoWriter(os.path.join(output_path, 'videos', f'{video_name}__fps{fps}.mp4'), cv2.VideoWriter_fourcc(*'MP4V'), fps, (w, h))\n",
        "\n",
        "    pose = []\n",
        "    left_hand = []\n",
        "    right_hand = []\n",
        "\n",
        "    hands_model = vision.HandLandmarker.create_from_options(hands_options)\n",
        "    pose_model = vision.PoseLandmarker.create_from_options(pose_options)\n",
        "\n",
        "    while cap.isOpened():\n",
        "        res, frame = cap.read()\n",
        "\n",
        "        if not res:\n",
        "            break\n",
        "\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frame.flags.writeable = False\n",
        "\n",
        "        t = int(time.time() * 1000)\n",
        "\n",
        "        pose_results = pose_model.detect_for_video(mp.Image(image_format=mp.ImageFormat.SRGB, data=frame), t)\n",
        "        hands_results = hands_model.detect_for_video(mp.Image(image_format=mp.ImageFormat.SRGB, data=frame), t)\n",
        "\n",
        "        pose_landmarks = get_pose_landmarks(pose_results)\n",
        "        pose.append(pose_landmarks)\n",
        "\n",
        "        left_hand_landmarks, right_hand_landmarks = get_hands_landmarks(hands_results)\n",
        "        left_hand.append(left_hand_landmarks)\n",
        "        right_hand.append(right_hand_landmarks)\n",
        "\n",
        "        frame.flags.writeable = True\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        draw_results(frame, pose_landmarks, left_hand_landmarks, right_hand_landmarks)\n",
        "\n",
        "        out.write(frame)\n",
        "        \n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    lm = to_results_dict(pose, left_hand, right_hand)\n",
        "    lm = {\n",
        "        key: np.array(value) for key, value in lm.items()\n",
        "    }\n",
        "\n",
        "    # We want to save the landmarks for left hand, right hand and pose,\n",
        "    # each having shape like:   (N, 21, 3),(N, 21, 3),    (N, 33, 3)\n",
        "    # Because they don't have the same shape, we pad left hand and right hand\n",
        "    # so we can save them all as a ndarray.\n",
        "    padding = ((0, 0), (0, lm['pose'].shape[1] - lm['left_hand'].shape[1]), (0, 0))\n",
        "    lm['left_hand'] = np.pad(lm['left_hand'], padding, 'constant', constant_values=0)\n",
        "    \n",
        "    padding = ((0, 0), (0, lm['pose'].shape[1] - lm['right_hand'].shape[1]), (0, 0))\n",
        "    lm['right_hand'] = np.pad(lm['right_hand'], padding, 'constant', constant_values=0)\n",
        "    \n",
        "    lm = np.array([lm['pose'], lm['left_hand'], lm['right_hand']], np.float32)\n",
        "    np.save(os.path.join(output_path, 'landmarks', f'{video_name}__fps{fps}.npy'), lm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_full_path(video_name):\n",
        "    pattern = r'__fps\\d+'\n",
        "    video_name = re.sub(pattern, '', video_name)\n",
        "    return os.path.join(scraped_dataset_path, video_name[0].upper(), video_name)\n",
        "\n",
        "\n",
        "scraped_videos = [os.path.join(root, file) for root, _, files in os.walk(scraped_dataset_path) for file in files]\n",
        "\n",
        "processed_video_names = os.listdir(os.path.join(output_path, 'videos'))\n",
        "processed_videos = [get_full_path(n) for n in processed_video_names]\n",
        "\n",
        "to_be_processed = os_sorted(set(scraped_videos).difference(set(processed_videos)))\n",
        "\n",
        "\n",
        "with concurrent.futures.ProcessPoolExecutor(max_workers=6) as executor:\n",
        "    futures = [executor.submit(process_video, video_path) for video_path in to_be_processed]\n",
        "\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        try:\n",
        "            future.result()\n",
        "        except Exception as e:\n",
        "            print(f'Exception: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload files to server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url = 'https://192.168.0.152:45455/File/'\n",
        "token = ''\n",
        "headers = {\n",
        "    'Authorization': f'Bearer {token}',\n",
        "}\n",
        "\n",
        "ssl_context = ssl.create_default_context()\n",
        "ssl_context.check_hostname = False\n",
        "ssl_context.verify_mode = ssl.CERT_NONE\n",
        "\n",
        "async def upload_file(session, directory, file_path):\n",
        "    mpwriter = aiohttp.MultipartWriter('form-data')\n",
        "    \n",
        "    f = open(file_path, 'rb')\n",
        "\n",
        "    try:\n",
        "        part = mpwriter.append(f)\n",
        "        file_name = os.path.basename(file_path)\n",
        "        part.headers['Content-Disposition'] = f'form-data; name=\"file\"; filename=\"{file_name}\"'\n",
        "        \n",
        "        async with session.post(url + directory, data=mpwriter, headers=headers, ssl=ssl_context) as response:\n",
        "            if not response.ok:\n",
        "                return file_path\n",
        "            return None\n",
        "    \n",
        "    except:\n",
        "        return file_path\n",
        "\n",
        "    finally:\n",
        "        f.close()\n",
        "\n",
        "\n",
        "async def upload_files(directory, files):\n",
        "    connector = aiohttp.TCPConnector(ssl=ssl_context)\n",
        "\n",
        "    async with aiohttp.ClientSession(connector=connector) as session:\n",
        "        tasks = [upload_file(session, directory, file) for file in files]\n",
        "        results = await asyncio.gather(*tasks)\n",
        "\n",
        "        return [result for result in results if result is not None]\n",
        "\n",
        "\n",
        "async def upload_landmarks():\n",
        "    landmarks_dir = os.path.join('assets', 'landmarks')\n",
        "    files = [os.path.join(landmarks_dir, f) for f in ['casă__fps30.npy', 'acasă__fps30.npy']]\n",
        "    fails = await upload_files(files)\n",
        "    print(fails)\n",
        "\n",
        "\n",
        "async def upload_models():\n",
        "    hand_model = os.path.join('assets', 'mp_models', 'hand_landmarker.task')\n",
        "    pose_model = os.path.join('assets', 'mp_models', 'pose_landmarker_lite.task')\n",
        "    gesture_model = os.path.join('..', 'gestures.tflite')\n",
        "\n",
        "    fails = await upload_files('model', [hand_model, pose_model, gesture_model])\n",
        "    print(fails)\n",
        "\n",
        "\n",
        "async def upload_classes():\n",
        "    fails = await upload_files('class', [os.path.join('..', 'gestures.json')])\n",
        "    print(fails)\n",
        "\n",
        "\n",
        "async def upload_chars():\n",
        "    chars_dir = os.path.join('assets', 'chars')\n",
        "    fails = await upload_files('char', [os.path.join(chars_dir, char) for char in os.listdir(chars_dir)])\n",
        "    print(fails)\n",
        "\n",
        "\n",
        "await upload_models()\n",
        "await upload_classes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kalman Filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from library.helpers.landmarks_smoother import _filter_useless, _get_non_zero_indices, _process\n",
        "\n",
        "\n",
        "landmark_dir = os.path.join('assets', 'landmarks', 'volan__fps25.npy')\n",
        "\n",
        "frames = np.load(landmark_dir)\n",
        "frames = _filter_useless(frames)\n",
        "\n",
        "right_hand = frames[2]\n",
        "start, stop = _get_non_zero_indices(right_hand)\n",
        "\n",
        "right_hand_kalman = np.copy(right_hand)\n",
        "_process(right_hand_kalman, 25, start, stop)\n",
        "\n",
        "right_hand = right_hand[start:stop]\n",
        "right_hand_kalman = right_hand_kalman[start:stop]\n",
        "\n",
        "pointer = right_hand[:, 8, :]\n",
        "pointer_kalman = right_hand_kalman[:, 8, :]\n",
        "\n",
        "time = np.linspace(0, len(pointer) / 25, len(pointer))\n",
        "\n",
        "x, y, z = zip(*pointer)\n",
        "xk, yk, zk = zip(*pointer_kalman)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "ax = fig.add_subplot(121, projection='3d')\n",
        "ax.set_ylim([0, 0.6])\n",
        "ax.set_zlim([0, 0.9])\n",
        "ax.plot(time, x, y, c='r', marker='o')\n",
        "ax.set_xlabel('Time (s)')\n",
        "ax.set_ylabel('X')\n",
        "ax.set_zlabel('Y')\n",
        "ax.zaxis.labelpad=-0.8\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "ax = fig.add_subplot(121, projection='3d')\n",
        "ax.set_ylim([0, 0.6])\n",
        "ax.set_zlim([0, 0.9])\n",
        "ax.plot(time, xk, yk, c='b', marker='s')\n",
        "ax.set_xlabel('Time (s)')\n",
        "ax.set_ylabel('X')\n",
        "ax.set_zlabel('Y')\n",
        "ax.zaxis.labelpad=-0.8\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from library.helpers.filters.moving_average import moving_average_smooth\n",
        "\n",
        "def plot_moving_average(fig, window_size, idx):\n",
        "    frames_copy = np.copy(frames)\n",
        "    right_hand = frames_copy[2]\n",
        "    _process(right_hand, 25, start, stop)\n",
        "\n",
        "    if window_size > 1:\n",
        "        frames_moving_average = [moving_average_smooth(value, window_size) for value in frames_copy]\n",
        "    else:\n",
        "        frames_moving_average = frames_copy\n",
        "    right_hand_moving_average = frames_moving_average[2]\n",
        "    right_hand_moving_average = right_hand_moving_average[start:stop]\n",
        "    pointer_moving_average = right_hand_moving_average[:, 8, :]\n",
        "\n",
        "    xma, yma, zma = zip(*pointer_moving_average)\n",
        "\n",
        "    ax = fig.add_subplot(4, 2, idx, projection='3d')\n",
        "    ax.set_ylim([0, 0.6])\n",
        "    ax.set_zlim([0, 0.9])\n",
        "    ax.plot(time, xma, yma, c='b', marker='s')\n",
        "    ax.set_xlabel('Time (s)')\n",
        "    ax.set_ylabel('X')\n",
        "    ax.set_zlabel('Y')\n",
        "    if window_size > 1:\n",
        "        ax.set_title(f'window_size={window_size}')\n",
        "    else:\n",
        "        ax.set_title(f'Original')\n",
        "    ax.zaxis.labelpad=-0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(11, 20))\n",
        "\n",
        "plot_moving_average(fig, 1, 1)\n",
        "plot_moving_average(fig, 3, 2)\n",
        "plot_moving_average(fig, 5, 3)\n",
        "plot_moving_average(fig, 7, 4)\n",
        "plot_moving_average(fig, 9, 5)\n",
        "plot_moving_average(fig, 11, 6)\n",
        "plot_moving_average(fig, 13, 7)\n",
        "plot_moving_average(fig, 15, 8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Web Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download all dlmg.ro videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url_dlmg = 'https://dlmg.ro/dictionar/'\n",
        "url_dlmg_words = 'https://dlmg.ro/ajax/?cat='\n",
        "\n",
        "dir_dlmg = os.path.join('.', 'scraped')\n",
        "if not os.path.exists(dir_dlmg):\n",
        "    os.mkdir(dir_dlmg)\n",
        "dir_dlmg = os.path.join(dir_dlmg, 'dmlg')\n",
        "if not os.path.exists(dir_dlmg):\n",
        "    os.mkdir(dir_dlmg)\n",
        "\n",
        "\n",
        "def get_letters_id_map():\n",
        "    letters_id_map = dict()\n",
        "\n",
        "    response = requests.get(url_dlmg)\n",
        "    if response.status_code != 200:\n",
        "        raise('Failed to retrieve the webpage.')\n",
        "\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    div_content = soup.find('div', class_='dictionar-cat-wrapper')\n",
        "    if not div_content:\n",
        "        raise('No \"dictionar-cat-wrapper\" class found in the HTML.')\n",
        "\n",
        "    anchors = div_content.find_all('a')\n",
        "    for a in anchors:\n",
        "        text = a.get_text()\n",
        "        onclick_text = a.get('onclick', '')\n",
        "        start = onclick_text.find(\"(\") + 1\n",
        "        end = onclick_text.find(\")\", start)\n",
        "        id = onclick_text[start:end]\n",
        "        \n",
        "        letters_id_map[text] = id\n",
        "\n",
        "    return letters_id_map\n",
        "\n",
        "\n",
        "def get_words_url_letter_map(letters_id_map):\n",
        "    words_url_letter_map = dict()\n",
        "\n",
        "    for letter, id in letters_id_map.items():\n",
        "        letter_path = os.path.join(dir_dlmg, letter)\n",
        "        \n",
        "        if not os.path.exists(letter_path):\n",
        "            os.mkdir(letter_path)\n",
        "\n",
        "        url_words = url_dlmg_words + id\n",
        "\n",
        "        response = requests.get(url_words)\n",
        "        if response.status_code != 200:\n",
        "            print('Failed to retrieve the webpage for letter: ', letter)\n",
        "            continue\n",
        "        \n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        anchors = soup.find_all('a')\n",
        "\n",
        "        for anchor in anchors:\n",
        "            a_text = anchor.get_text()\n",
        "            if not a_text in words_url_letter_map:\n",
        "                words_url_letter_map[a_text] = {'url': anchor['href'], 'letter': letter }\n",
        "\n",
        "    return words_url_letter_map\n",
        "\n",
        "\n",
        "async def download_video(session, word, url, letter):\n",
        "    word_path = os.path.join(dir_dlmg, letter, word)\n",
        "\n",
        "    try:\n",
        "        async with session.get(url) as response:\n",
        "            if response.status != 200:\n",
        "                print('Failed to retrieve the webpage for video: ', word)\n",
        "                return\n",
        "            \n",
        "            text = await response.text()\n",
        "            soup = BeautifulSoup(text, 'html.parser')\n",
        "            video_tag = soup.find('source', {'type': 'video/mp4'})\n",
        "\n",
        "            if not video_tag or 'src' not in video_tag.attrs:\n",
        "                print('Failed to retrieve the video URL for:', word)\n",
        "                return\n",
        "\n",
        "            video_url = video_tag['src']\n",
        "\n",
        "            async with session.get(video_url) as video_response:\n",
        "                if video_response.status != 200:\n",
        "                    print('Failed to download video:', word)\n",
        "                    return\n",
        "\n",
        "                with open(word_path + '.mp4', 'wb') as file:\n",
        "                    async for chunk in video_response.content.iter_chunked(1024 * 1024):\n",
        "                        file.write(chunk)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'An error occurred for {word}: {str(e)}')\n",
        "\n",
        "\n",
        "async def download_videos(words_url_letter_map):\n",
        "    connector = TCPConnector(limit=16)\n",
        "    async with aiohttp.ClientSession(connector=connector) as session:\n",
        "        tasks = []\n",
        "        for word, dictionary in words_url_letter_map.items():\n",
        "            url = dictionary['url']\n",
        "            letter = dictionary['letter']\n",
        "            task = download_video(session, word, url, letter)\n",
        "            tasks.append(task)\n",
        "        \n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "\n",
        "letters_id_map = get_letters_id_map()\n",
        "words_url_letter_map = get_words_url_letter_map(letters_id_map)\n",
        "await download_videos(words_url_letter_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Download all pesemne.ro videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "url_pesemne = 'https://pesemne.ro/'\n",
        "url_pesemne_clips = url_pesemne + 'wp-content/uploads/clips/'\n",
        "\n",
        "dir_pesemne = os.path.join('.', 'scraped')\n",
        "if not os.path.exists(dir_pesemne):\n",
        "    os.mkdir(dir_pesemne)\n",
        "dir_pesemne = os.path.join(dir_pesemne, 'pesemne')\n",
        "if not os.path.exists(dir_pesemne):\n",
        "    os.mkdir(dir_pesemne)\n",
        "\n",
        "\n",
        "response = requests.get(url_pesemne)\n",
        "if response.status_code != 200:\n",
        "    raise('Failed to retrieve the webpage.')\n",
        "\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "ul_element = soup.find('ul', id='manual-selection-list')\n",
        "if not ul_element:\n",
        "    raise ValueError('Failed to parse the webpage.')\n",
        "li_elements = ul_element.find_all('li')\n",
        "\n",
        "data_words = []\n",
        "data_clips = []\n",
        "\n",
        "for li in li_elements:\n",
        "    try:\n",
        "        data_word = li.get('data-word', None)\n",
        "        if not data_word:\n",
        "            raise ValueError()\n",
        "        \n",
        "        data_clip = li.get('data-clip', None)\n",
        "        if not data_clip:\n",
        "            raise ValueError()\n",
        "        \n",
        "        data_words.append(data_word)\n",
        "        data_clips.append(data_clip)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "async def download_video(session, data_word, data_clip):\n",
        "    word_clean = data_word.removeprefix('a ')\n",
        "\n",
        "    if word_clean == '':\n",
        "        return\n",
        "\n",
        "    letter_path = os.path.join(dir_pesemne, word_clean[0].upper())\n",
        "    if not os.path.exists(letter_path):\n",
        "        os.mkdir(letter_path)\n",
        "\n",
        "    word_path = os.path.join(letter_path, word_clean)\n",
        "\n",
        "    try:\n",
        "        async with session.get(url_pesemne_clips + data_clip) as video_response:\n",
        "            if video_response.status != 200:\n",
        "                print('Failed to download video:', data_word)\n",
        "                return\n",
        "\n",
        "            with open(word_path + '.mp4', 'wb') as file:\n",
        "                async for chunk in video_response.content.iter_chunked(1024 * 1024):\n",
        "                    file.write(chunk)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'An error occurred for {data_word}: {str(e)}')\n",
        "\n",
        "\n",
        "async def download_videos():\n",
        "    connector = TCPConnector(limit=16)\n",
        "    async with aiohttp.ClientSession(connector=connector) as session:\n",
        "        tasks = []\n",
        "        for data_word, data_clip in zip(data_words, data_clips):\n",
        "            task = download_video(session, data_word, data_clip)\n",
        "            tasks.append(task)\n",
        "        \n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "\n",
        "await download_videos()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Sentence to lexemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import Levenshtein as lev\n",
        "import os\n",
        "import spacy_stanza\n",
        "import sys\n",
        "\n",
        "from collections import Counter\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "\n",
        "directory = landmarks_path\n",
        "\n",
        "\n",
        "nlp = spacy_stanza.load_pipeline(\"ro\")\n",
        "stemmer = SnowballStemmer(\"romanian\")\n",
        "\n",
        "\n",
        "vocabulary = [file_name[:file_name.find('__fps')] for file_name in os.listdir(directory)]\n",
        "\n",
        "\n",
        "def cosine_dist(a, b):\n",
        "    a_vals = Counter(a)\n",
        "    b_vals = Counter(b)\n",
        "\n",
        "    chars = list(a_vals.keys() | b_vals.keys())\n",
        "    a_vect = [a_vals.get(c, 0) for c in chars]\n",
        "    b_vect = [b_vals.get(c, 0) for c in chars]\n",
        "\n",
        "    len_a = sum(av * av for av in a_vect) ** 0.5\n",
        "    len_b = sum(bv * bv for bv in b_vect) ** 0.5\n",
        "    dot = sum(av * bv for av, bv in zip(a_vect, b_vect))\n",
        "    cosine = dot / (len_a * len_b)\n",
        "\n",
        "    return 1 - cosine\n",
        "\n",
        "\n",
        "def find_closest_words(doc, is_end):\n",
        "    doc_len = len(doc)\n",
        "\n",
        "    def search_in_vocab(word):\n",
        "        distance_map = dict()\n",
        "\n",
        "        for vocab_word in vocabulary:\n",
        "            max_len = max(1, len(word) // 2)\n",
        "            start_a = word[:max_len]\n",
        "            start_b = vocab_word[:max_len]\n",
        "\n",
        "            if start_a == start_b:\n",
        "                dist = lev.distance(vocab_word, word)\n",
        "            else:\n",
        "                dist = sys.maxsize\n",
        "                \n",
        "            distance_map[vocab_word] = dist\n",
        "        \n",
        "        min_dist = min(distance_map.values())\n",
        "        \n",
        "        candidates = [key for key, value in distance_map.items() if value == min_dist]\n",
        "        candidates = sorted(candidates, key=lambda x: cosine_dist(word, x))\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if min_dist < int(len(candidate) / 2):\n",
        "                return candidate\n",
        "        \n",
        "        return None\n",
        "    \n",
        "\n",
        "    def process_token(token, idx):\n",
        "        token_text = token.text.lower()\n",
        "        token_lemma = token.lemma_.lower()\n",
        "        token_stem = stemmer.stem(token.text).lower()\n",
        "\n",
        "        # weirdness\n",
        "        if token_text == 'm':\n",
        "            if idx < doc_len - 1 and doc[idx + 1].pos_ == 'AUX':\n",
        "                return ['eu']\n",
        "            else:\n",
        "                return ['m']\n",
        "\n",
        "        elif token_text == 'mânc' or token_text == 'mănânc':\n",
        "            return ['mânca']\n",
        "            \n",
        "        elif token_text == 'n':\n",
        "            if idx < doc_len - 1 and (doc[idx + 1].pos_ == 'ADV' or doc[idx + 1].pos_ == 'NOUN'):\n",
        "                return ['în']\n",
        "            elif idx >= 1 and (doc[idx - 1].pos_ == 'PRON' or doc[idx - 1].pos_ == 'VERB'):\n",
        "                return ['în']\n",
        "            else:\n",
        "                return ['n']\n",
        "            \n",
        "        elif token_text == 'ă':\n",
        "            return ['ă']\n",
        "\n",
        "        # connection words\n",
        "        elif token_text == 'dar':\n",
        "            if 'CONJ' in token.pos_:\n",
        "                return ['dar (conjuncție)']\n",
        "            else:\n",
        "                return ['dar (cadou)']\n",
        "            \n",
        "        elif token_text == 'ori':\n",
        "            if 'CONJ' in token.pos_:\n",
        "                return ['sau']\n",
        "            else:\n",
        "                return ['ori']\n",
        "            \n",
        "        elif token_text == 'fie':\n",
        "            if 'CONJ' in token.pos_:\n",
        "                return ['sau']\n",
        "            else:\n",
        "                return ['fi']\n",
        "            \n",
        "        elif token_text == 'că':\n",
        "            return []\n",
        "            \n",
        "        elif token_text == 'ci':\n",
        "            return []\n",
        "            \n",
        "        elif token_text == 'de':\n",
        "            if idx < doc_len - 1 and doc[idx + 1].text == 'ce':\n",
        "                return []\n",
        "            else:\n",
        "                return ['de']\n",
        "            \n",
        "        elif token_text == 'ce':\n",
        "            if idx >= 1 and doc[idx - 1].text == 'de':\n",
        "                return ['de ce']\n",
        "            else:\n",
        "                return ['ce']\n",
        "            \n",
        "        elif token_text == 'la':\n",
        "            if idx < doc_len - 1 and doc[idx + 1].text == 'revedere':\n",
        "                return []\n",
        "            else:\n",
        "                return ['la']\n",
        "            \n",
        "        elif token_text == 'revedere':\n",
        "            if idx >= 1 and doc[idx - 1].text == 'la':\n",
        "                return ['la revedere']\n",
        "            else:\n",
        "                return ['vedea']\n",
        "            \n",
        "        elif token_text == 'deși':\n",
        "            return ['chiar', 'dacă']\n",
        "        \n",
        "        elif token_text == 'da':\n",
        "            if token.pos_ == 'ADV':\n",
        "                return ['da (adverb)']\n",
        "            else:\n",
        "                return ['da (verb)']\n",
        "            \n",
        "        elif (token.pos_ == 'AUX' or token.pos_ == 'PART') and idx < doc_len - 1 and (doc[idx + 1].pos_ == 'VERB' or doc[idx + 1].pos_ == 'AUX' or doc[idx + 1].pos_ == 'PART'):\n",
        "            return []\n",
        "            \n",
        "        elif token.pos_ == 'DET' and idx < doc_len - 1 and doc[idx + 1].pos_ == 'NUM':\n",
        "            return []\n",
        "        \n",
        "        elif token_text == 'mai' and idx < doc_len - 1 and (doc[idx + 1].text.lower() == 'un' or doc[idx + 1].text.lower() == 'o'):\n",
        "            return ['încă']\n",
        "\n",
        "        # adjectives, nouns, verbs\n",
        "        elif token_lemma == 'da':\n",
        "            return ['da (verb)']\n",
        "        \n",
        "        elif token.pos_ == 'VERB':\n",
        "            word = search_in_vocab(token_lemma)\n",
        "            if word:\n",
        "                return [word]\n",
        "            word = search_in_vocab(token_stem)\n",
        "            if word:\n",
        "                return [word]\n",
        "            else:\n",
        "                return [x for x in token_lemma]\n",
        "        \n",
        "        elif token.pos_ == 'ADJ' or token.pos_ == 'NOUN':\n",
        "            word = search_in_vocab(token_lemma)\n",
        "            if word:\n",
        "                return [word]\n",
        "            word = search_in_vocab(token_text)\n",
        "            if word:\n",
        "                return [word]\n",
        "            word = search_in_vocab(token_stem)\n",
        "            if word:\n",
        "                return [word]\n",
        "            else:\n",
        "                return [x for x in token_lemma]\n",
        "            \n",
        "        # proper name\n",
        "        elif token.pos_ == 'PROPN':\n",
        "            return [x for x in token_text]\n",
        "            \n",
        "        # numeral\n",
        "        elif token.pos_ == 'NUM' or (token.pos_ == 'X' and any(char.isdigit() for char in token_text)):\n",
        "            # only digits\n",
        "            if '.' in token_text:\n",
        "                token_text = token_text.replace('.', '')\n",
        "                if ',' in token_text:\n",
        "                    split = token_text.split(',')\n",
        "                    res = []\n",
        "                    res.extend([x for x in split[0]])\n",
        "                    res.append('virgulă')\n",
        "                    res.extend([x for x in split[1]])\n",
        "                    return res\n",
        "\n",
        "            # from 1M up, there are digits and letter\n",
        "            # (e.g. un milion, 100 (de) milioane)\n",
        "            word = search_in_vocab(token_text)\n",
        "            if word:\n",
        "                return [word]\n",
        "            word = search_in_vocab(token_lemma)\n",
        "            if word:\n",
        "                return [word]\n",
        "            else:\n",
        "                return [x for x in token_text]\n",
        "            \n",
        "        # pronoun:\n",
        "        elif token.pos_ == \"PRON\":\n",
        "            if token_text == 'eu' or token_text == 'mine' or token_text == 'mă' or token_text == 'mie' or token_text == 'îmi' or token_text == 'mi':\n",
        "                return ['eu']\n",
        "            elif token_text == 'tu' or token_text == 'tine' or token_text == 'te' or token_text == 'ție' or token_text == 'îți' or token_text == 'ți':\n",
        "                return ['tu']\n",
        "            elif token_text == 'el' or token_text == 'îl' or token_text == 'l' or token_text == 'lui' or token_text == 'îi' or token_text == 'i':\n",
        "                return ['el']\n",
        "            elif token_text == 'ea' or token_text == 'o':\n",
        "                return ['ea']\n",
        "            elif token_text == 'noi' or token_text == 'ne' or token_text == 'nouă' or token_text == 'ni':\n",
        "                return ['noi']\n",
        "            elif token_text == 'voi' or token_text == 'vă' or token_text == 'vouă' or token_text == 'vi':\n",
        "                return ['voi']\n",
        "            elif token_text == 'ei' or token_text == 'îi' or token_text == 'i' or token_text == 'lor' or token_text == 'le' or token_text == 'li':\n",
        "                return ['ei']\n",
        "            elif token_text == 'ele' or token_text == 'le':\n",
        "                return ['ele']\n",
        "            elif token_text == 'unul' or token_text == 'una':\n",
        "                return ['un']\n",
        "            else:\n",
        "                word = search_in_vocab(token_lemma)\n",
        "                if word:\n",
        "                    return [word]\n",
        "                else:\n",
        "                    return []\n",
        "            \n",
        "        # determiner:\n",
        "        elif token.pos_ == \"DET\":\n",
        "            if token_text == 'un' or token_text == 'o':\n",
        "                return ['un']\n",
        "            elif token_text == 'meu' or token_text == 'mea' or token_text == 'mei' or token_text == 'mele':\n",
        "                return ['meu']\n",
        "            elif token_text == 'tău' or token_text == 'ta':\n",
        "                return ['tău']\n",
        "            elif token_text == 'lui':\n",
        "                return ['el']\n",
        "            elif token_text == 'ei':\n",
        "                return ['ea']\n",
        "            elif token_text == 'său' or token_text == 'sa':\n",
        "                return ['său']\n",
        "            elif token_text == 'nostru':\n",
        "                return ['nostru']\n",
        "            elif token_text == 'vostru':\n",
        "                return ['vostru']\n",
        "            elif token_text == 'lor':\n",
        "                return ['ei']\n",
        "            else:\n",
        "                word = search_in_vocab(token_lemma)\n",
        "                if word:\n",
        "                    return [word]\n",
        "                else:\n",
        "                    return []\n",
        "\n",
        "        else:\n",
        "            word = search_in_vocab(token_lemma)\n",
        "            if word:\n",
        "                return [word]\n",
        "            else:\n",
        "                return []\n",
        "\n",
        "    if is_end:\n",
        "        res_0 = process_token(doc[-3], doc_len - 3)\n",
        "        res_1 = process_token(doc[-2], doc_len - 2)\n",
        "        res_2 = process_token(doc[-1], doc_len - 1)\n",
        "        return res_0, res_1, res_2\n",
        "    else:\n",
        "        res_0 = process_token(doc[-3], doc_len - 3)\n",
        "        return res_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sentence_to_lexemes(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    doc_len = len(doc)\n",
        "\n",
        "    while doc_len < 3:\n",
        "        sentence += \" .\"\n",
        "        doc = nlp(sentence)\n",
        "        doc_len = len(doc)\n",
        "\n",
        "    print(sentence)\n",
        "\n",
        "    for i in range(doc_len + 1):\n",
        "        if (i < 3):\n",
        "            continue\n",
        "\n",
        "        print(find_closest_words(doc[:i], i == doc_len))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence_to_lexemes(\"Deși era obosit, Ioan a continuat să lucreze până târziu în noapte pentru a finaliza raportul detaliat cerut de superiorii săi.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Azi dimineață am alergat în parc și m-am bucurat de aerul proaspăt.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Calul paște\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"a b c d e f g h i j k l m n o p q r s t u v w x y z ă î ș ț â\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Mă doare capul\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Merg acasă\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Cam atât a fost\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Această propoziție este în limba română.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"A măsura este foarte bine.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Măsurătoarea cuiva fără măsură este nulă.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"În alergarea sa, alergătorul aleargă foarte repede.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"În alergarea ei, alergătoarea aleargă foarte repede.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"În alergarea lui, alergătorul aleargă foarte repede.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"a înroși, înroșire, roșiatic, înroșit, roșeață, Marea Roșie, piei-roșii, pătlăgică-roșie, coadă-roșie, roșul\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"mă numesc Radu\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"În 1992 o casă costa 10 milioane de lei vechi.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"a mers acolo\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Ea este frumoasă\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Pe noi ne cheamă tatăl ei acasă.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Cine e la ușă?\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Conduc din Arad in Timișoara.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Una din noi va pleca.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Unul din nou va pleca.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Pentru că sunt frumos, mă uit in oglindă.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Tu ești ca mine.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Ești ori prost ori te prefaci.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"5 ori 5 egal 25\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Nu ăsta, ci celălalt.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Ce ai zis?\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"În timp ce vizitam clădirea, am căzut.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Da, m-am dus acolo!\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Cândva am făcut și asta!\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Câți băieți ați fost acolo?\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"A o da în bară uneori este normal\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Ei vor merge mâine.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Vor merge mâine.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Va merge mâine.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"123.456\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Ea se spală singură.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Mă spăl singur.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Mă spăl pe mine.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Mă spăl\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"O fata e aici.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Acest om e aici.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Acesta e aici.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Ăsta e aici.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Calul meu\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Am mers acolo\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"M-am dus acolo\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Eu m-am dus acolo\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Cui i-ai luat cadoul?\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"El este german.\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Dar de ce ai facut asta?\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"Dar de ce și de unde ai facut asta?\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes(\"La anul și la revedere\")\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes('Mai o data')\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes('Da o n colo')\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes('Cine n spanac face asta')\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes('Uită te n vas')\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes('M am supărat')\n",
        "print()\n",
        "\n",
        "sentence_to_lexemes('M ai lovit')\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
