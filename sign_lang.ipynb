{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sign Language Translator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install tensorflow==2.15\n",
        "\n",
        "%matplotlib widget\n",
        "import concurrent\n",
        "import cv2\n",
        "import json\n",
        "import mediapipe as mp\n",
        "import numpy as np\n",
        "import os\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "from collections import namedtuple\n",
        "from keras.callbacks import TensorBoard, LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import BatchNormalization, LSTM, Dense\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from matplotlib import pyplot as plt\n",
        "from mediapipe import solutions\n",
        "from mediapipe.tasks import python\n",
        "from mediapipe.tasks.python import vision\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from translate_inv.library.skeleton import Skeleton\n",
        "from translate_inv.library.calc_R import calc_R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hands_base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "hands_options = vision.HandLandmarkerOptions(base_options=hands_base_options,\n",
        "                                       running_mode=mp.tasks.vision.RunningMode.VIDEO,\n",
        "                                       num_hands=2,\n",
        "                                       min_hand_detection_confidence=0.6,\n",
        "                                       min_hand_presence_confidence=0.6,\n",
        "                                       min_tracking_confidence=0.5)\n",
        "\n",
        "pose_base_options = python.BaseOptions(model_asset_path='pose_landmarker_lite.task')\n",
        "pose_options = vision.PoseLandmarkerOptions(base_options=pose_base_options,\n",
        "                                       running_mode=mp.tasks.vision.RunningMode.VIDEO,\n",
        "                                       min_pose_detection_confidence=0.7,\n",
        "                                       min_pose_presence_confidence=0.7,\n",
        "                                       min_tracking_confidence=0.5)\n",
        "\n",
        "# Reference skeleton\n",
        "skeleton = Skeleton(os.path.join('translate_inv', 'assets', 'chars', 'Xbot.glb'))\n",
        "\n",
        "# Path for dataset\n",
        "dataset_path = os.path.join('dataset')\n",
        "\n",
        "# Path for processed dataset\n",
        "processed_dataset_path = os.path.join('processed_dataset')\n",
        "\n",
        "# Classes\n",
        "classes = sorted(os.listdir(processed_dataset_path))\n",
        "\n",
        "# Path for test dataset\n",
        "test_dataset_path = os.path.join('test_dataset')\n",
        "\n",
        "# List of gestures in the dataset\n",
        "gestures = ['W']\n",
        "\n",
        "# Number of records for one gesture\n",
        "num_records = 30\n",
        "\n",
        "# USB webcam\n",
        "cam_idx = 0\n",
        "\n",
        "# Recording is done on Ubuntu\n",
        "api_pref = cv2.CAP_V4L2\n",
        "\n",
        "# Target frame resolution\n",
        "target_width = 640\n",
        "target_height = 480\n",
        "\n",
        "# Record FPS and dt\n",
        "record_fps = 30\n",
        "record_dt = 1 / record_fps\n",
        "\n",
        "# Inference FPS\n",
        "inference_fps = int(record_fps / 2)\n",
        "\n",
        "# Tolerance between dt and record_dt\n",
        "dt_tol = 0.002\n",
        "\n",
        "# Delay for getting into position\n",
        "record_delay = 1\n",
        "\n",
        "# Path where we save the gesture classifier weights\n",
        "gestures_weights_path = 'gestures'\n",
        "converted_gestures_weights_path = 'gestures.tflite'\n",
        "\n",
        "# Num workers\n",
        "num_workers = 6\n",
        "\n",
        "# No hand landmarks\n",
        "no_hand_landmarks_list = [[0] * 3 for _ in range(21)]\n",
        "no_hand_landmarks_ndarray = np.array(no_hand_landmarks_list)\n",
        "\n",
        "# No arm landmarks\n",
        "no_pose_landmarks_ndarray = np.array([[0] * 3 for _ in range(33)])\n",
        "\n",
        "# Inference parameters\n",
        "min_unchanged_ms = 600\n",
        "mostly_empty_percentage = 0.7\n",
        "num_hand_features = 21 * 3\n",
        "prediction_threshold = 0.9\n",
        "\n",
        "HandLandmark = solutions.hands.HandLandmark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_hands_landmarks(results, relative_to_hand_center):\n",
        "    left = [None]\n",
        "    right = [None]\n",
        "\n",
        "    results_hand_landmarks = results.hand_world_landmarks if relative_to_hand_center else results.hand_landmarks\n",
        "    LandmarkListType = landmark_pb2.LandmarkList if relative_to_hand_center else landmark_pb2.NormalizedLandmarkList\n",
        "    LandmarkType = landmark_pb2.Landmark if relative_to_hand_center else landmark_pb2.NormalizedLandmark\n",
        "\n",
        "    # handedness list and hand_landmarks list match by index\n",
        "    for handedness, hand_landmarks in zip(results.handedness, results_hand_landmarks):\n",
        "        handedness = handedness[0]\n",
        "\n",
        "        hand_landmarks_proto = LandmarkListType()\n",
        "        hand_landmarks_proto.landmark.extend(\n",
        "            [LandmarkType(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks]\n",
        "            )\n",
        "        \n",
        "        # Assume we are pointing the camera to a person with one left and one right hand.\n",
        "        # That means that the only correct predicted handednesses are { 'Left' AND 'Right' }\n",
        "\n",
        "        # If predicted handedness is left\n",
        "        if handedness.category_name == 'Left':\n",
        "            # If left was NOT predicted before\n",
        "            if left[0] is None:\n",
        "                # Just assign the landmarks and handedness probability\n",
        "                left = [hand_landmarks_proto, handedness.score]\n",
        "            # Else, if we have a collision\n",
        "            else:\n",
        "                # If current prediction is more accurate:\n",
        "                if handedness.score > left[1]:\n",
        "                    # That means that the previous left value is actually right\n",
        "                    right = left\n",
        "                    left = [hand_landmarks_proto, handedness.score]\n",
        "                # Else, if current prediction is less accurate\n",
        "                else:\n",
        "                    # That means that the current prediction is actually right\n",
        "                    right = [hand_landmarks_proto, handedness.score]\n",
        "        \n",
        "        # Same exact thing for right\n",
        "        if handedness.category_name == 'Right':\n",
        "            if right[0] is None:\n",
        "                right = [hand_landmarks_proto, handedness.score]\n",
        "            else:\n",
        "                if handedness.score > right[1]:\n",
        "                    left = right\n",
        "                    right = [hand_landmarks_proto, handedness.score]\n",
        "                else:\n",
        "                    left = [hand_landmarks_proto, handedness.score]\n",
        "\n",
        "    return left[0], right[0]\n",
        "\n",
        "\n",
        "def get_pose_landmarks(results, relative_to_hips_center):\n",
        "    pose = None\n",
        "\n",
        "    results_pose_landmarks = results.pose_world_landmarks if relative_to_hips_center else results.pose_landmarks\n",
        "    LandmarkListType = landmark_pb2.LandmarkList if relative_to_hips_center else landmark_pb2.NormalizedLandmarkList\n",
        "    LandmarkType = landmark_pb2.Landmark if relative_to_hips_center else landmark_pb2.NormalizedLandmark\n",
        "\n",
        "    for pose_landmarks in results_pose_landmarks:\n",
        "        pose_landmarks_proto = LandmarkListType()\n",
        "        pose_landmarks_proto.landmark.extend(\n",
        "            [LandmarkType(x=landmark.x, y=landmark.y, z=landmark.z, visibility=landmark.visibility) for landmark in pose_landmarks]\n",
        "            )\n",
        "        \n",
        "        pose = pose_landmarks_proto\n",
        "\n",
        "    return pose\n",
        "\n",
        "\n",
        "def draw_hands_landmarks(image, left_hand_landmarks, right_hand_landmarks):\n",
        "    if left_hand_landmarks is not None:\n",
        "        solutions.drawing_utils.draw_landmarks(\n",
        "            image,\n",
        "            left_hand_landmarks,\n",
        "            solutions.hands.HAND_CONNECTIONS,\n",
        "            solutions.drawing_utils.DrawingSpec((0, 0, 128), thickness=2, circle_radius=4), \n",
        "            solutions.drawing_utils.DrawingSpec((65, 105, 225), thickness=2, circle_radius=2))\n",
        "    \n",
        "    if right_hand_landmarks is not None:\n",
        "        solutions.drawing_utils.draw_landmarks(\n",
        "            image,\n",
        "            right_hand_landmarks,\n",
        "            solutions.hands.HAND_CONNECTIONS,\n",
        "            solutions.drawing_utils.DrawingSpec((128, 0, 0), thickness=2, circle_radius=4), \n",
        "            solutions.drawing_utils.DrawingSpec((225, 105, 65), thickness=2, circle_radius=2))\n",
        "        \n",
        "    return image\n",
        "\n",
        "\n",
        "def draw_pose_landmarks(image, pose_landmarks):\n",
        "    if pose_landmarks is not None:\n",
        "        solutions.drawing_utils.draw_landmarks(\n",
        "            image,\n",
        "            pose_landmarks,\n",
        "            solutions.pose.POSE_CONNECTIONS,\n",
        "            solutions.drawing_utils.DrawingSpec((0, 128, 0), thickness=2, circle_radius=4), \n",
        "            solutions.drawing_utils.DrawingSpec((105, 225, 65), thickness=2, circle_radius=2))\n",
        "        \n",
        "    return image\n",
        "  \n",
        "\n",
        "def get_landmarks_dict(\n",
        "        pose_landmarks,\n",
        "        pose_world_landmarks,\n",
        "        left_hand_landmarks,\n",
        "        left_hand_world_landmarks,\n",
        "        right_hand_landmarks,\n",
        "        right_hand_world_landmarks):\n",
        "    # 33 pose landmarks => array with 3 * 33 elements\n",
        "    pose = [[p.x, p.y, p.z] for p in pose_landmarks.landmark] if pose_landmarks is not None else [[0] * 3 for _ in range(33)]\n",
        "    # 33 pose landmarks => array with 3 * 33 elements\n",
        "    pose_world = [[p.x, p.y, p.z] for p in pose_world_landmarks.landmark] if pose_world_landmarks is not None else [[0] * 3 for _ in range(33)]\n",
        "    # 21 hand landmarks => array with 3 * 21 elements\n",
        "    lh = [[l.x, l.y, l.z] for l in left_hand_landmarks.landmark] if left_hand_landmarks is not None else no_hand_landmarks_list\n",
        "    # 21 hand landmarks => array with 3 * 21 elements\n",
        "    lh_world = [[l.x, l.y, l.z] for l in left_hand_world_landmarks.landmark] if left_hand_world_landmarks is not None else no_hand_landmarks_list\n",
        "    # 21 hand landmarks => array with 3 * 21 elements\n",
        "    rh = [[r.x, r.y, r.z] for r in right_hand_landmarks.landmark] if right_hand_landmarks is not None else no_hand_landmarks_list\n",
        "    # 21 hand landmarks => array with 3 * 21 elements\n",
        "    rh_world = [[r.x, r.y, r.z] for r in right_hand_world_landmarks.landmark] if right_hand_world_landmarks is not None else no_hand_landmarks_list\n",
        "    return {\n",
        "        'pose': pose,\n",
        "        'pose_world': pose_world,\n",
        "        'lh': lh,\n",
        "        'lh_world': lh_world,\n",
        "        'rh': rh,\n",
        "        'rh_world': rh_world\n",
        "        }\n",
        "\n",
        "\n",
        "def plot_landmarks(ax, landmarks, connections, landmarks_color=(1, 0, 0), connections_color=(0, 0, 0), marker='o'):\n",
        "  plotted_landmarks = {}\n",
        "\n",
        "  for idx, landmark in enumerate(landmarks):\n",
        "    ax.scatter(\n",
        "        xs=[landmark[0]],\n",
        "        ys=[landmark[1]],\n",
        "        zs=[landmark[2]],\n",
        "        color=landmarks_color,\n",
        "        marker=marker,\n",
        "        linewidth=2)\n",
        "    plotted_landmarks[idx] = (landmark[0], landmark[1], landmark[2])\n",
        "\n",
        "  if connections:\n",
        "    num_landmarks = len(landmarks)\n",
        "\n",
        "    # Draws the connections if the start and end landmarks are both visible.\n",
        "    for connection in connections:\n",
        "      start_idx = connection[0]\n",
        "      end_idx = connection[1]\n",
        "\n",
        "      if not (0 <= start_idx < num_landmarks and 0 <= end_idx < num_landmarks):\n",
        "        raise ValueError(f'Landmark index is out of range. Invalid connection '\n",
        "                         f'from landmark #{start_idx} to landmark #{end_idx}.')\n",
        "      \n",
        "      if start_idx in plotted_landmarks and end_idx in plotted_landmarks:\n",
        "        landmark_pair = [\n",
        "            plotted_landmarks[start_idx], plotted_landmarks[end_idx]\n",
        "        ]\n",
        "        ax.plot(\n",
        "            xs=[landmark_pair[0][0], landmark_pair[1][0]],\n",
        "            ys=[landmark_pair[0][1], landmark_pair[1][1]],\n",
        "            zs=[landmark_pair[0][2], landmark_pair[1][2]],\n",
        "            color=connections_color,\n",
        "            linewidth=2)\n",
        "\n",
        "\n",
        "def get_normalized_by_depth(v, z_origin):\n",
        "    if z_origin == 0:\n",
        "        return np.zeros(v.shape)\n",
        "\n",
        "    return v / z_origin\n",
        "\n",
        "\n",
        "def _get_normalized_displacement(prev, curr, z_origin_prev, z_origin_curr):    \n",
        "    prev = get_normalized_by_depth(prev, z_origin_prev)\n",
        "    curr = get_normalized_by_depth(curr, z_origin_curr)\n",
        "\n",
        "    d = curr - prev\n",
        "\n",
        "    if not d.any():\n",
        "        return d\n",
        "\n",
        "    return d / np.linalg.norm(d)\n",
        "\n",
        "\n",
        "def get_pose_normalized_displacement(prev, curr):\n",
        "    if not prev.any():\n",
        "        return prev\n",
        "    if not curr.any():\n",
        "        return curr\n",
        "    \n",
        "    mid_hips_prev = (prev[23] + prev[24]) / 2\n",
        "    mid_hips_curr = (curr[23] + curr[24]) / 2\n",
        "\n",
        "    return _get_normalized_displacement(prev, curr, mid_hips_prev[2], mid_hips_curr[2])\n",
        "\n",
        "\n",
        "def get_hand_normalized_displacement(prev, curr):\n",
        "    if not prev.any():\n",
        "        return prev\n",
        "    if not curr.any():\n",
        "        return curr\n",
        "    \n",
        "    wrist_prev = prev[0]\n",
        "    wrist_curr = curr[0]\n",
        "\n",
        "    return _get_normalized_displacement(prev, curr, wrist_prev[2], wrist_curr[2])\n",
        "\n",
        "\n",
        "def tflite_predict(interpreter, input):\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    interpreter.set_tensor(input_details[0]['index'], input)\n",
        "    interpreter.invoke()\n",
        "    return interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "\n",
        "def lerp(first, second, t=0.5):\n",
        "    return (1 - t) * first + t * second\n",
        "\n",
        "\n",
        "def create_normalized_landmark_list(points):\n",
        "    landmark_list = landmark_pb2.NormalizedLandmarkList()\n",
        "    \n",
        "    for point in points:\n",
        "        landmark = landmark_list.landmark.add()\n",
        "        landmark.x, landmark.y, landmark.z = point\n",
        "    \n",
        "    return landmark_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create dataset folder structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset folder\n",
        "if not os.path.isdir(dataset_path):\n",
        "    os.makedirs(dataset_path)\n",
        "\n",
        "# Create gesture subfolders\n",
        "for gesture in gestures:\n",
        "    gesture_dir = os.path.join(dataset_path, gesture)\n",
        "    if not os.path.isdir(gesture_dir):\n",
        "        os.makedirs(gesture_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Record data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(cam_idx, api_pref)\n",
        "\n",
        "# Set target resolution\n",
        "cap.set(cv2.CAP_PROP_FRAME_WIDTH, target_width)\n",
        "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, target_height)\n",
        "\n",
        "# Make sure OpenCV set it right\n",
        "print(f'width: {cap.get(cv2.CAP_PROP_FRAME_WIDTH)}')\n",
        "print(f'height: {cap.get(cv2.CAP_PROP_FRAME_HEIGHT)}')\n",
        "\n",
        "# Compute dt helpers\n",
        "prev_time = 0\n",
        "cur_time = 0\n",
        "\n",
        "# Flag for gracefully exiting\n",
        "running = True\n",
        "\n",
        "# Flag for starting recording\n",
        "start = False\n",
        "\n",
        "\n",
        "def get_dt():\n",
        "    global prev_time, cur_time\n",
        "\n",
        "    cur_time = time.time()\n",
        "    dt = cur_time - prev_time\n",
        "    prev_time = cur_time\n",
        "\n",
        "    return dt\n",
        "\n",
        "\n",
        "def wait_quit():\n",
        "    global running\n",
        "\n",
        "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "        running = False\n",
        "\n",
        "\n",
        "# Record for 1 second at record_fps\n",
        "def record_gesture(gesture, record_idx):\n",
        "    global running\n",
        "\n",
        "    record_dir = os.path.join(dataset_path, gesture, str(record_idx))\n",
        "    if not os.path.isdir(record_dir):\n",
        "        os.makedirs(record_dir)\n",
        "    \n",
        "    frame_num = 0\n",
        "    while running and frame_num < record_fps:\n",
        "        # Read frame\n",
        "        _, frame = cap.read()\n",
        "\n",
        "        dt = get_dt()\n",
        "\n",
        "        file_path = os.path.join(record_dir, f'{frame_num}.jpg')\n",
        "        cv2.imwrite(file_path, frame)\n",
        "        frame_num += 1\n",
        "\n",
        "        # Mirror the frame\n",
        "        frame = cv2.flip(frame, 1)\n",
        "\n",
        "        # Show FPS on frame\n",
        "        fps = round(1 / dt)\n",
        "        cv2.putText(frame, str(fps), (5, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 0))\n",
        "\n",
        "        # Show frame on screen\n",
        "        cv2.imshow('Camera', frame)\n",
        "        wait_quit()\n",
        "\n",
        "\n",
        "def wait_start_or_quit():\n",
        "    global start, running\n",
        "\n",
        "    key_pressed = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "    if key_pressed == ord('s'):\n",
        "        start = True\n",
        "    elif key_pressed == ord('q'):\n",
        "        running = False\n",
        "       \n",
        "        \n",
        "gesture = 'Test'\n",
        "record_num = 0\n",
        "\n",
        "while running:\n",
        "    start = False\n",
        "\n",
        "    # Read frame\n",
        "    _, frame = cap.read()\n",
        "\n",
        "    # Mirror the frame\n",
        "    frame = cv2.flip(frame, 1)\n",
        "\n",
        "    # Show helper text on frame\n",
        "    cv2.putText(frame, f'Now recording `{gesture}`', (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 0, 0), 2)\n",
        "    cv2.putText(frame, f'{record_num} / {num_records - 1}', (5, 85), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 0, 0), 2)\n",
        "    cv2.putText(frame, 'Press `s` to start recording', (10, 135), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 0, 0), 2)\n",
        "\n",
        "    # Show frame\n",
        "    cv2.imshow('Camera', frame)\n",
        "    wait_start_or_quit()\n",
        "\n",
        "    if start:\n",
        "        t1 = time.time()\n",
        "\n",
        "        while True:\n",
        "            t2 = time.time()\n",
        "            \n",
        "            # Read frame\n",
        "            _, frame = cap.read()\n",
        "            \n",
        "            # Mirror the frame\n",
        "            frame = cv2.flip(frame, 1)\n",
        "\n",
        "            # Show helper text on frame\n",
        "            delay_left = round(record_delay - (t2 - t1), 2)\n",
        "            cv2.putText(frame, f'Starting in {delay_left} seconds', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.4, (0, 0, 0), 2)\n",
        "\n",
        "            # Show frame on screen\n",
        "            cv2.imshow('Camera', frame)\n",
        "            wait_quit()\n",
        "\n",
        "            if not running or t2 - t1 >= record_delay:\n",
        "                break\n",
        "\n",
        "        if running:\n",
        "            record_gesture(gesture, record_num)\n",
        "            record_num += 1\n",
        "\n",
        "    if record_num == num_records:\n",
        "        break\n",
        "\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create processed dataset folder structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create processed dataset folder\n",
        "if not os.path.isdir(processed_dataset_path):\n",
        "    os.makedirs(processed_dataset_path)\n",
        "\n",
        "# Create gesture subfolders\n",
        "for gesture in os.listdir(dataset_path):\n",
        "    gesture_dir = os.path.join(processed_dataset_path, gesture)\n",
        "    if not os.path.isdir(gesture_dir):\n",
        "        os.makedirs(gesture_dir)\n",
        "\n",
        "    # Create recording sub-subfolder\n",
        "    for record_num in range(num_records):\n",
        "        record_dir = os.path.join(gesture_dir, str(record_num))\n",
        "        if not os.path.isdir(record_dir):\n",
        "            os.makedirs(record_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Extract and save landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_gesture(gesture):\n",
        "    gesture_dir = os.path.join(dataset_path, gesture)\n",
        "    processed_gesture_dir = os.path.join(processed_dataset_path, gesture)\n",
        "\n",
        "    for record_num in os.listdir(gesture_dir):\n",
        "        record_dir = os.path.join(gesture_dir, record_num)\n",
        "        processed_record_dir = os.path.join(processed_gesture_dir, record_num)\n",
        "\n",
        "        hands_model = vision.HandLandmarker.create_from_options(hands_options)\n",
        "        pose_model = vision.PoseLandmarker.create_from_options(pose_options)\n",
        "        timestamp = 0\n",
        "\n",
        "        for frame_num in range(record_fps):\n",
        "            frame_name = f'{frame_num}.jpg'\n",
        "\n",
        "            # Read frame from file\n",
        "            frame = cv2.imread(os.path.join(record_dir, frame_name))\n",
        "\n",
        "            # Convert to RGB\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            frame.flags.writeable = False\n",
        "            \n",
        "            # Perform detection\n",
        "            hands_results = hands_model.detect_for_video(mp.Image(image_format=mp.ImageFormat.SRGB, data=frame), timestamp)\n",
        "            pose_results = pose_model.detect_for_video(mp.Image(image_format=mp.ImageFormat.SRGB, data=frame), timestamp)\n",
        "            timestamp += int(record_dt * 1000)\n",
        "\n",
        "            frame.flags.writeable = True\n",
        "\n",
        "            # Get hands landmarks\n",
        "            left_hand_landmarks, right_hand_landmarks = get_hands_landmarks(hands_results, False)\n",
        "            left_hand_world_landmarks, right_hand_world_landmarks = get_hands_landmarks(hands_results, True)\n",
        "\n",
        "            # Draw hands landmarks on frame\n",
        "            draw_hands_landmarks(frame, left_hand_landmarks, right_hand_landmarks)\n",
        "\n",
        "            # Get pose landmarks\n",
        "            pose_landmarks = get_pose_landmarks(pose_results, False)\n",
        "            pose_world_landmarks = get_pose_landmarks(pose_results, True)\n",
        "\n",
        "            # Draw arms landmarks on frame\n",
        "            draw_pose_landmarks(frame, pose_landmarks)\n",
        "\n",
        "            # Convert back to BGR\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "            # Save frame with landmarks\n",
        "            cv2.imwrite(os.path.join(processed_record_dir, frame_name), frame)\n",
        "\n",
        "            # Extract landmarks\n",
        "            landmarks = get_landmarks_dict(\n",
        "                pose_landmarks,\n",
        "                pose_world_landmarks,\n",
        "                left_hand_landmarks,\n",
        "                left_hand_world_landmarks,\n",
        "                right_hand_landmarks,\n",
        "                right_hand_world_landmarks)\n",
        "\n",
        "            with open(os.path.join(processed_record_dir, f'{frame_num}.json'), 'w') as fout:\n",
        "                json.dump(landmarks, fout)\n",
        "\n",
        "\n",
        "with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "    futures = [executor.submit(process_gesture, gesture) for gesture in gestures]\n",
        "\n",
        "    for future in concurrent.futures.as_completed(futures):\n",
        "        try:\n",
        "            future.result()\n",
        "        except Exception as e:\n",
        "            print(f'Exception: {e}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Check for wrongly processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "two_handed = ['A avea', 'A da', 'A face', 'A găsi', 'A simți', 'A trăi', 'Casă', 'Q', 'W', 'X']\n",
        "\n",
        "for gesture in os.listdir(processed_dataset_path):\n",
        "    for i in range(num_records):\n",
        "        record_path = os.path.join(dataset_path, gesture, str(i))\n",
        "        processed_record_path = os.path.join(processed_dataset_path, gesture, str(i))\n",
        "        bad_frames = []\n",
        "\n",
        "        for j in range(record_fps):\n",
        "            frame_path = os.path.join(processed_record_path, f'{j}.json')\n",
        "\n",
        "            with open(frame_path) as fin:\n",
        "                frame_dict = json.load(fin)\n",
        "\n",
        "                if gesture in two_handed:\n",
        "                    if frame_dict['lh'] == no_hand_landmarks_list or frame_dict['rh'] == no_hand_landmarks_list:\n",
        "                        bad_frames.append(j)\n",
        "                    elif frame_dict['lh_world'] == no_hand_landmarks_list or frame_dict['rh_world'] == no_hand_landmarks_list:\n",
        "                        bad_frames.append(j)\n",
        "                    else:\n",
        "                        if j == 0:\n",
        "                            first = 1\n",
        "                            second = 2\n",
        "                        elif j == record_fps - 1:\n",
        "                            first = record_fps - 3\n",
        "                            second = record_fps - 2\n",
        "                        else:\n",
        "                            first = j - 1\n",
        "                            second = j + 1\n",
        "\n",
        "                        first = os.path.join(processed_record_path, f'{first}.json')\n",
        "                        with open(first) as fin:\n",
        "                            first = json.load(fin)\n",
        "\n",
        "                        second = os.path.join(processed_record_path, f'{second}.json')\n",
        "                        with open(second) as fin:\n",
        "                            second = json.load(fin)\n",
        "                            \n",
        "                        rr_dist_first = np.sum(np.linalg.norm(np.array(frame_dict['rh']) - np.array(first['rh']), axis=1))\n",
        "                        rl_dist_first = np.sum(np.linalg.norm(np.array(frame_dict['rh']) - np.array(first['lh']), axis=1))\n",
        "                            \n",
        "                        rr_dist_second = np.sum(np.linalg.norm(np.array(frame_dict['rh']) - np.array(second['rh']), axis=1))\n",
        "                        rl_dist_second = np.sum(np.linalg.norm(np.array(frame_dict['rh']) - np.array(second['lh']), axis=1))\n",
        "\n",
        "                        if rr_dist_first > rl_dist_first and rr_dist_second > rl_dist_second:\n",
        "                            bad_frames.append(j)\n",
        "\n",
        "\n",
        "                else:\n",
        "                    if frame_dict['lh'] != no_hand_landmarks_list:\n",
        "                        bad_frames.append(j)\n",
        "                    elif frame_dict['rh'] == no_hand_landmarks_list:\n",
        "                        bad_frames.append(j)\n",
        "                    elif frame_dict['lh_world'] != no_hand_landmarks_list:\n",
        "                        bad_frames.append(j)\n",
        "                    elif frame_dict['rh_world'] == no_hand_landmarks_list:\n",
        "                        bad_frames.append(j)\n",
        "\n",
        "        if len(bad_frames) != 0:\n",
        "            print(f'Fixing: {processed_record_path}')\n",
        "            print(f'Fixing: {bad_frames}')\n",
        "            consec = any(bad_frames[j] + 1 == bad_frames[j + 1] for j in range(len(bad_frames) - 1))\n",
        "            if consec or 0 in bad_frames and 2 in bad_frames or (record_fps - 3) in bad_frames and (record_fps - 1) in bad_frames:\n",
        "                print('Could not fix\\n')\n",
        "            else:\n",
        "                if 0 in bad_frames:\n",
        "                    next = os.path.join(processed_record_path, '1.json')\n",
        "                    with open(next) as fin:\n",
        "                        next = json.load(fin)\n",
        "                        \n",
        "                    next_next = os.path.join(processed_record_path, '2.json')\n",
        "                    with open(next_next) as fin:\n",
        "                        next_next = json.load(fin)\n",
        "\n",
        "                    cur = {\n",
        "                        k: lerp(np.array(next[k]), np.array(next_next[k]), -0.5).tolist() for k in next.keys()\n",
        "                    }\n",
        "                    \n",
        "                    frame = cv2.imread(os.path.join(record_path, '0.jpg'))\n",
        "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                    draw_hands_landmarks(frame, create_normalized_landmark_list(cur['lh']), create_normalized_landmark_list(cur['rh']))\n",
        "                    draw_pose_landmarks(frame, create_normalized_landmark_list(cur['pose']))\n",
        "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    cv2.imwrite(os.path.join(processed_record_path, '0.jpg'), frame)\n",
        "\n",
        "                    with open(os.path.join(processed_record_path, '0.json'), 'w') as fout:\n",
        "                        json.dump(cur, fout)\n",
        "\n",
        "                if record_fps - 1 in bad_frames:\n",
        "                    prev_prev = os.path.join(processed_record_path, f'{record_fps - 3}.json')\n",
        "                    with open(prev_prev) as fin:\n",
        "                        prev_prev = json.load(fin)\n",
        "                        \n",
        "                    prev = os.path.join(processed_record_path, f'{record_fps - 2}.json')\n",
        "                    with open(prev) as fin:\n",
        "                        prev = json.load(fin)\n",
        "\n",
        "                    cur = {\n",
        "                        k: lerp(np.array(prev_prev[k]), np.array(prev[k]), 2).tolist() for k in prev.keys()\n",
        "                    }\n",
        "                    \n",
        "                    frame = cv2.imread(os.path.join(record_path, f'{record_fps - 1}.jpg'))\n",
        "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                    draw_hands_landmarks(frame, create_normalized_landmark_list(cur['lh']), create_normalized_landmark_list(cur['rh']))\n",
        "                    draw_pose_landmarks(frame, create_normalized_landmark_list(cur['pose']))\n",
        "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    cv2.imwrite(os.path.join(processed_record_path, f'{record_fps - 1}.jpg'), frame)\n",
        "\n",
        "                    with open(os.path.join(processed_record_path, f'{record_fps - 1}.json'), 'w') as fout:\n",
        "                        json.dump(cur, fout)\n",
        "\n",
        "                bad_frames = [j for j in bad_frames if j != 0 and j != record_fps - 1]\n",
        "\n",
        "                for j in bad_frames:\n",
        "                    prev = os.path.join(processed_record_path, f'{j - 1}.json')\n",
        "                    with open(prev) as fin:\n",
        "                        prev = json.load(fin)\n",
        "                        \n",
        "                    next = os.path.join(processed_record_path, f'{j + 1}.json')\n",
        "                    with open(next) as fin:\n",
        "                        next = json.load(fin)\n",
        "\n",
        "                    cur = {\n",
        "                        k: lerp(np.array(prev[k]), np.array(next[k])).tolist() for k in prev.keys()\n",
        "                    }\n",
        "                    \n",
        "                    frame = cv2.imread(os.path.join(record_path, f'{j}.jpg'))\n",
        "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "                    draw_hands_landmarks(frame, create_normalized_landmark_list(cur['lh']), create_normalized_landmark_list(cur['rh']))\n",
        "                    draw_pose_landmarks(frame, create_normalized_landmark_list(cur['pose']))\n",
        "                    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "                    cv2.imwrite(os.path.join(processed_record_path, f'{j}.jpg'), frame)\n",
        "\n",
        "                    with open(os.path.join(processed_record_path, f'{j}.json'), 'w') as fout:\n",
        "                        json.dump(cur, fout)\n",
        "\n",
        "\n",
        "\n",
        "def manual_fix(record_path, first, second):\n",
        "    t_values = np.linspace(0, 1, second - first + 1)\n",
        "\n",
        "    first_frame = os.path.join(record_path, f'{first}.json')\n",
        "    with open(first_frame) as fin:\n",
        "        first_frame = json.load(fin)\n",
        "        \n",
        "    second_frame = os.path.join(record_path, f'{second}.json')\n",
        "    with open(second_frame) as fin:\n",
        "        second_frame = json.load(fin)\n",
        "\n",
        "    lerp_lh = [lerp(np.array(first_frame['lh']), np.array(second_frame['lh']), t) for t in t_values]\n",
        "    lerp_lh_world = [lerp(np.array(first_frame['lh_world']), np.array(second_frame['lh_world']), t) for t in t_values]\n",
        "    lerp_rh = [lerp(np.array(first_frame['rh']), np.array(second_frame['rh']), t) for t in t_values]\n",
        "    lerp_rh_world = [lerp(np.array(first_frame['rh_world']), np.array(second_frame['rh_world']), t) for t in t_values]\n",
        "    lerp_pose = [lerp(np.array(first_frame['pose']), np.array(second_frame['pose']), t) for t in t_values]\n",
        "    lerp_pose_world = [lerp(np.array(first_frame['pose_world']), np.array(second_frame['pose_world']), t) for t in t_values]\n",
        "\n",
        "    for i, (l, lw, r, rw, p, pw) in enumerate(zip(lerp_lh, lerp_lh_world, lerp_rh, lerp_rh_world, lerp_pose, lerp_pose_world)):\n",
        "        dictionary = dict()\n",
        "        dictionary['lh'] = l.tolist()\n",
        "        dictionary['lh_world'] = lw.tolist()\n",
        "        dictionary['rh'] = r.tolist()\n",
        "        dictionary['rh_world'] = rw.tolist()\n",
        "        dictionary['pose'] = p.tolist()\n",
        "        dictionary['pose_world'] = pw.tolist()\n",
        "        with open(os.path.join(record_path, f'{first + i}.json'), 'w') as fout:\n",
        "            json.dump(dictionary, fout)\n",
        "\n",
        "        frame = cv2.imread(os.path.join(record_path.replace('processed_', ''), f'{first + i}.jpg'))\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        draw_hands_landmarks(frame, create_normalized_landmark_list(dictionary['lh']), create_normalized_landmark_list(dictionary['rh']))\n",
        "        draw_pose_landmarks(frame, create_normalized_landmark_list(dictionary['pose']))\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "        cv2.imwrite(os.path.join(record_path, f'{first + i}.jpg'), frame)\n",
        "\n",
        "\n",
        "# manual_fix(os.path.join(processed_dataset_path, 'Z', '28'), 19, 24)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preview processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "running = True\n",
        "\n",
        "for gesture in sorted(os.listdir(processed_dataset_path)):\n",
        "    if not running:\n",
        "        break\n",
        "\n",
        "    for i in range(num_records):\n",
        "        if not running:\n",
        "            break\n",
        "\n",
        "        for j in range(record_fps):\n",
        "            if not running:\n",
        "                break\n",
        "\n",
        "            frame_path = os.path.join(processed_dataset_path, gesture, str(i), f'{j}.jpg')\n",
        "            print(frame_path)\n",
        "\n",
        "            # Read frame from file\n",
        "            frame = cv2.imread(frame_path)\n",
        "            cv2.imshow('Camera', frame)\n",
        "            if cv2.waitKey(int(record_dt * 250)) & 0xFF == ord('q'):\n",
        "                running = False\n",
        "        \n",
        "        if cv2.waitKey(250) & 0xFF == ord('q'):\n",
        "            running = False\n",
        "\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "running = True\n",
        "\n",
        "for i in range(num_records):\n",
        "    if not running:\n",
        "        break\n",
        "\n",
        "    for gesture in os.listdir(processed_dataset_path):\n",
        "        if not running:\n",
        "            break\n",
        "\n",
        "        for j in range(record_fps):\n",
        "            if not running:\n",
        "                break\n",
        "\n",
        "            frame_path = os.path.join(dataset_path, gesture, str(i), f'{j}.jpg')\n",
        "            print(frame_path)\n",
        "\n",
        "            # Read frame from file\n",
        "            frame = cv2.imread(frame_path)\n",
        "            cv2.imshow('Camera', frame)\n",
        "            if cv2.waitKey(int(record_dt * 1000)) & 0xFF == ord('q'):\n",
        "                running = False\n",
        "\n",
        "            cv2.waitKey(20)\n",
        "        \n",
        "        if cv2.waitKey(500) & 0xFF == ord('q'):\n",
        "            running = False\n",
        "\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Extract some sample data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hands_base_options = python.BaseOptions(model_asset_path='hand_landmarker.task')\n",
        "hands_options = vision.HandLandmarkerOptions(base_options=hands_base_options,\n",
        "                                       running_mode=mp.tasks.vision.RunningMode.IMAGE,\n",
        "                                       num_hands=2,\n",
        "                                       min_hand_detection_confidence=0.4,\n",
        "                                       min_hand_presence_confidence=0.4,\n",
        "                                       min_tracking_confidence=0.4)\n",
        "hands_model = vision.HandLandmarker.create_from_options(hands_options)\n",
        "\n",
        "directory = 'casa'\n",
        "names = ['0', '0_close', '0_far', '1', '1_close', '1_far']\n",
        "\n",
        "for name in names:\n",
        "    file = os.path.join(directory, name)\n",
        "\n",
        "    frame = cv2.imread(file + '.jpg')\n",
        "\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    result = hands_model.detect(mp.Image(image_format=mp.ImageFormat.SRGB, data=frame))\n",
        "    left_hand_landmarks, right_hand_landmarks = get_hands_landmarks(result, False)\n",
        "    left_hand_world_landmarks, right_hand_world_landmarks = get_hands_landmarks(result, True)\n",
        "\n",
        "    # Draw hands landmarks on frame\n",
        "    draw_hands_landmarks(frame, left_hand_landmarks, right_hand_landmarks)\n",
        "\n",
        "    # Convert back to BGR\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Save frame with landmarks\n",
        "    cv2.imwrite(file + '_out.jpg', frame)\n",
        "\n",
        "    # Extract landmarks\n",
        "    landmarks = get_landmarks_dict(\n",
        "        None,\n",
        "        None,\n",
        "        left_hand_landmarks,\n",
        "        left_hand_world_landmarks,\n",
        "        right_hand_landmarks,\n",
        "        right_hand_world_landmarks)\n",
        "\n",
        "    with open(file + '.json', 'w') as fout:\n",
        "        json.dump(landmarks, fout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot landmarks and world landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "landmarks = None\n",
        "\n",
        "with open(os.path.join('urca', '17_close.json')) as fin:\n",
        "    landmarks = json.load(fin)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "\n",
        "ax = fig.add_subplot(121, projection='3d')\n",
        "ax.view_init(elev=240, azim=270)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('z')\n",
        "ax.set_title('Landmarks')\n",
        "plot_landmarks(ax, landmarks['rh'], solutions.hands_connections.HAND_CONNECTIONS)\n",
        "\n",
        "ax = fig.add_subplot(122, projection='3d')\n",
        "ax.view_init(elev=240, azim=270)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('z')\n",
        "ax.set_title('World landmarks')\n",
        "plot_landmarks(ax, landmarks['rh_world'], solutions.hands_connections.HAND_CONNECTIONS)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot hand movement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_helper(title, start, finish, rows, columns, idx):\n",
        "    ax = fig.add_subplot(rows, columns, idx, projection='3d')\n",
        "    ax.view_init(elev=240, azim=270)\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    ax.set_zlabel('z')\n",
        "    ax.set_title(title)\n",
        "    plot_landmarks(ax, start, solutions.hands_connections.HAND_CONNECTIONS, (1, 0, 0), marker='o')\n",
        "    plot_landmarks(ax, finish, solutions.hands_connections.HAND_CONNECTIONS, (0, 0, 1), marker='s')\n",
        "    for s, f in zip(start, finish):\n",
        "        ax.plot([s[0], f[0]], [s[1], f[1]], [s[2], f[2]], 'k:')\n",
        "\n",
        "    print(title + f'\\n{np.linalg.norm(finish - start)}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(os.path.join('urca', '17.json')) as fin:\n",
        "    start = np.array(json.load(fin)['rh'])\n",
        "    start_norm = get_normalized_by_depth(start, start[0][2])\n",
        "\n",
        "with open(os.path.join('urca', '18.json')) as fin:\n",
        "    finish = np.array(json.load(fin)['rh'])\n",
        "    finish_norm = get_normalized_by_depth(finish, finish[0][2])\n",
        "\n",
        "with open(os.path.join('urca', '17_close.json')) as fin:\n",
        "    start_close = np.array(json.load(fin)['rh'])\n",
        "    start_close_norm = get_normalized_by_depth(start_close, start_close[0][2])\n",
        "\n",
        "with open(os.path.join('urca', '18_close.json')) as fin:\n",
        "    finish_close = np.array(json.load(fin)['rh'])\n",
        "    finish_close_norm = get_normalized_by_depth(finish_close, finish_close[0][2])\n",
        "\n",
        "with open(os.path.join('urca', '17_far.json')) as fin:\n",
        "    start_far = np.array(json.load(fin)['rh'])\n",
        "    start_far_norm = get_normalized_by_depth(start_far, start_far[0][2])\n",
        "\n",
        "with open(os.path.join('urca', '18_far.json')) as fin:\n",
        "    finish_far = np.array(json.load(fin)['rh'])\n",
        "    finish_far_norm = get_normalized_by_depth(finish_far, finish_far[0][2])\n",
        "\n",
        "\n",
        "rows = 9\n",
        "columns = 1\n",
        "fig = plt.figure(figsize=(5 * columns, 5 * rows))\n",
        "        \n",
        "plot_helper('Distance between start and finish', start_norm, finish_norm, rows, columns, 1)\n",
        "plot_helper('Distance between start_close and finish_close', start_close_norm, finish_close_norm, rows, columns, 2)\n",
        "plot_helper('Distance between start_far and finish_far', start_far_norm, finish_far_norm, rows, columns, 3)\n",
        "\n",
        "plot_helper('Distance between start_close and finish_far norm', start_close_norm, finish_far_norm, rows, columns, 5)\n",
        "        \n",
        "plot_helper('Distance between start and finish', start, finish, rows, columns, 7)\n",
        "plot_helper('Distance between start_close and finish_close', start_close, finish_close, rows, columns, 8)\n",
        "plot_helper('Distance between start_far and finish_far', start_far, finish_far, rows, columns, 9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot thumb and palm normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "landmarks = None\n",
        "\n",
        "with open(os.path.join(sample_dir, '0_close.json')) as fin:\n",
        "    landmarks = np.array(json.load(fin)['rh'])\n",
        "\n",
        "palm_normal = get_palm_normal(landmarks)\n",
        "thumb_normal = get_thumb_normal(landmarks)\n",
        "\n",
        "fig = plt.figure(figsize=(15, 15))\n",
        "\n",
        "ax = fig.add_subplot(121, projection='3d')\n",
        "ax.view_init(elev=270, azim=90)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.set_zlabel('z')\n",
        "ax.set_title('Landmarks with palm normal')\n",
        "plot_landmarks(ax, landmarks, solutions.hands_connections.HAND_CONNECTIONS)\n",
        "ax.quiver(*landmarks[HandLandmark.WRIST], *palm_normal)\n",
        "ax.quiver(*landmarks[HandLandmark.WRIST], *thumb_normal, color='red')\n",
        "\n",
        "for point, angle in zip([landmarks[HandLandmark.THUMB_TIP], landmarks[HandLandmark.INDEX_FINGER_TIP], landmarks[HandLandmark.MIDDLE_FINGER_TIP], landmarks[HandLandmark.RING_FINGER_TIP], landmarks[HandLandmark.PINKY_TIP]], get_hand_angles(landmarks)):\n",
        "    ax.text(*point, str(round(angle, 2)))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a gesture classification model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# (15, 33, 3)\n",
        "def get_pose_record_displacements(record, prev):\n",
        "    displacements = []\n",
        "\n",
        "    for frame in record:\n",
        "        displacements.append(get_pose_normalized_displacement(prev, frame))\n",
        "        prev = frame\n",
        "\n",
        "    return np.array(displacements)\n",
        "\n",
        "\n",
        "# (15, 21, 3)\n",
        "def get_hand_record_displacements(record, prev):\n",
        "    displacements = []\n",
        "\n",
        "    for frame in record:\n",
        "        displacements.append(get_hand_normalized_displacement(prev, frame))\n",
        "        prev = frame\n",
        "\n",
        "    return np.array(displacements)\n",
        "\n",
        "\n",
        "# (15, 19, 4), (15, 19, 4)\n",
        "def get_record_rotations(pose, left_hand, right_hand):\n",
        "    lh_result = []\n",
        "    rh_result = []\n",
        "\n",
        "    for p, lh, rh in zip(pose, left_hand, right_hand):\n",
        "        rotations = calc_R(skeleton, p, lh, rh)\n",
        "\n",
        "        lh_result.append(rotations[3:22])\n",
        "        rh_result.append(rotations[22:41])\n",
        "\n",
        "    return np.array(lh_result), np.array(rh_result)\n",
        "\n",
        "\n",
        "# (21, 3), (21, 3), (21, 3), (21, 3), (33, 3), (33, 3)\n",
        "def extract_landmarks(path):\n",
        "    with open(path) as fin:\n",
        "        landmarks_dict = json.load(fin)\n",
        "        return landmarks_dict['lh_world'],\\\n",
        "               landmarks_dict['lh'],\\\n",
        "               landmarks_dict['rh_world'],\\\n",
        "               landmarks_dict['rh'],\\\n",
        "               landmarks_dict['pose_world'],\\\n",
        "               landmarks_dict['pose']\n",
        "\n",
        "\n",
        "# { h: (15, 21, 3) },  { p: (15, 33, 3) }\n",
        "def get_two_records(dir):\n",
        "    lh_wld_one = []\n",
        "    lh_one = []\n",
        "    rh_wld_one = []\n",
        "    rh_one = []\n",
        "    p_wld_one = []\n",
        "    p_one = []\n",
        "\n",
        "    lh_wld_two = []\n",
        "    lh_two = []\n",
        "    rh_wld_two = []\n",
        "    rh_two = []\n",
        "    p_wld_two = []\n",
        "    p_two = []\n",
        "\n",
        "    for frame_num in range(0, record_fps, 2):\n",
        "        lh_wld, lh, rh_wld, rh, p_wld, p = extract_landmarks(os.path.join(dir, f'{frame_num}.json'))\n",
        "        lh_wld_one.append(lh_wld)\n",
        "        lh_one.append(lh)\n",
        "        rh_wld_one.append(rh_wld)\n",
        "        rh_one.append(rh)\n",
        "        p_wld_one.append(p_wld)\n",
        "        p_one.append(p)\n",
        "\n",
        "        lh_wld, lh, rh_wld, rh, p_wld, p = extract_landmarks(os.path.join(dir, f'{frame_num + 1}.json'))\n",
        "        lh_wld_two.append(lh_wld)\n",
        "        lh_two.append(lh)\n",
        "        rh_wld_two.append(rh_wld)\n",
        "        rh_two.append(rh)\n",
        "        p_wld_two.append(p_wld)\n",
        "        p_two.append(p)\n",
        "\n",
        "    return {\n",
        "        'lh_wld_one': np.array(lh_wld_one),\n",
        "        'lh_one': np.array(lh_one),\n",
        "        'rh_wld_one': np.array(rh_wld_one),\n",
        "        'rh_one': np.array(rh_one),\n",
        "        'p_wld_one': np.array(p_wld_one),\n",
        "        'p_one': np.array(p_one),\n",
        "        'lh_wld_two': np.array(lh_wld_two),\n",
        "        'lh_two': np.array(lh_two),\n",
        "        'rh_wld_two': np.array(rh_wld_two),\n",
        "        'rh_two': np.array(rh_two),\n",
        "        'p_wld_two': np.array(p_wld_two),\n",
        "        'p_two': np.array(p_two)\n",
        "    }\n",
        "\n",
        "\n",
        "def preprocess_data():\n",
        "    # Encode text label to integer helper map\n",
        "    labels_map = { label : num for num, label in enumerate(classes) }\n",
        "\n",
        "    labels = []\n",
        "    # (N, 15, 440)\n",
        "    features = []\n",
        "\n",
        "    for gesture in classes:\n",
        "        processed_gesture_dir = os.path.join(processed_dataset_path, gesture)\n",
        "\n",
        "        for record_num in range(num_records):\n",
        "            labels.append(labels_map[gesture])\n",
        "            labels.append(labels_map[gesture])\n",
        "            \n",
        "            # { hands: (15, 21, 3) },  { pose: (15, 33, 3) }\n",
        "            records = get_two_records(os.path.join(processed_gesture_dir, str(record_num)))\n",
        "\n",
        "            # (15, 21, 3)\n",
        "            lh_wld_one = records['lh_wld_one']\n",
        "            # (15, 21, 3)\n",
        "            rh_wld_one = records['rh_wld_one']\n",
        "            # (15, 3, 3)\n",
        "            la_wld_one = records['p_wld_one'][:, [11, 13, 15], :]\n",
        "            # (15, 3, 3)\n",
        "            ra_wld_one = records['p_wld_one'][:, [12, 14, 16], :]\n",
        "            # (15, 21, 3)\n",
        "            lh_wld_two = records['lh_wld_two']\n",
        "            # (15, 21, 3)\n",
        "            rh_wld_two = records['rh_wld_two']\n",
        "            # (15, 3, 3)\n",
        "            la_wld_two = records['p_wld_two'][:, [11, 13, 15], :]\n",
        "            # (15, 3, 3)\n",
        "            ra_wld_two = records['p_wld_two'][:, [12, 14, 16], :]\n",
        "\n",
        "            # (15, 21, 3)\n",
        "            lh_one_diffs = get_hand_record_displacements(records['lh_one'], no_hand_landmarks_ndarray)\n",
        "            # (15, 21, 3)\n",
        "            rh_one_diffs = get_hand_record_displacements(records['rh_one'], no_hand_landmarks_ndarray)\n",
        "            # (15, 3, 3)\n",
        "            la_one_diffs = get_pose_record_displacements(records['p_one'], no_pose_landmarks_ndarray)[:, [11, 13, 15], :]\n",
        "            # (15, 3, 3)\n",
        "            ra_one_diffs = get_pose_record_displacements(records['p_one'], no_pose_landmarks_ndarray)[:, [12, 14, 16], :]\n",
        "            # (15, 21, 3)\n",
        "            lh_two_diffs = get_hand_record_displacements(records['lh_two'], no_hand_landmarks_ndarray)\n",
        "            # (15, 21, 3)\n",
        "            rh_two_diffs = get_hand_record_displacements(records['rh_two'], no_hand_landmarks_ndarray)\n",
        "            # (15, 3, 3)\n",
        "            la_two_diffs = get_pose_record_displacements(records['p_two'], no_pose_landmarks_ndarray)[:, [11, 13, 15], :]\n",
        "            # (15, 3, 3)\n",
        "            ra_two_diffs = get_pose_record_displacements(records['p_two'], no_pose_landmarks_ndarray)[:, [12, 14, 16], :]\n",
        "\n",
        "            # (15, 19, 4)\n",
        "            lh_one_rotations, rh_one_rotations = get_record_rotations(records['p_one'], records['lh_one'], records['rh_one'])\n",
        "            lh_two_rotations, rh_two_rotations = get_record_rotations(records['p_two'], records['lh_two'], records['rh_two'])\n",
        "\n",
        "            # (15, 440)\n",
        "            f = [np.concatenate([\n",
        "                    lh_wld_one[i].reshape(-1),\n",
        "                    rh_wld_one[i].reshape(-1),\n",
        "                    la_wld_one[i].reshape(-1),\n",
        "                    ra_wld_one[i].reshape(-1),\n",
        "                    lh_one_diffs[i].reshape(-1),\n",
        "                    rh_one_diffs[i].reshape(-1),\n",
        "                    la_one_diffs[i].reshape(-1),\n",
        "                    ra_one_diffs[i].reshape(-1),\n",
        "                    lh_one_rotations[i].reshape(-1),\n",
        "                    rh_one_rotations[i].reshape(-1)\n",
        "                ]) for i in range(inference_fps)]\n",
        "            features.append(f)\n",
        "\n",
        "            # (15, 440)\n",
        "            f = [np.concatenate([\n",
        "                    lh_wld_two[i].reshape(-1),\n",
        "                    rh_wld_two[i].reshape(-1),\n",
        "                    la_wld_two[i].reshape(-1),\n",
        "                    ra_wld_two[i].reshape(-1),\n",
        "                    lh_two_diffs[i].reshape(-1),\n",
        "                    rh_two_diffs[i].reshape(-1),\n",
        "                    la_two_diffs[i].reshape(-1),\n",
        "                    ra_two_diffs[i].reshape(-1),\n",
        "                    lh_two_rotations[i].reshape(-1),\n",
        "                    rh_two_rotations[i].reshape(-1)\n",
        "                ]) for i in range(inference_fps)]\n",
        "            features.append(f)\n",
        "\n",
        "    X = np.array(features)\n",
        "    print(np.shape(X))\n",
        "\n",
        "    # One hot encoding\n",
        "    y = to_categorical(labels).astype(int)\n",
        "    print(np.shape(y))\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create labels, split train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X, y = preprocess_data()\n",
        "\n",
        "# 80% train, 20% rest\n",
        "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2)\n",
        "print(np.shape(X_train))\n",
        "# 50% test, 50% validate\n",
        "X_test, X_validate, y_test, y_validate = train_test_split(X_rest, y_rest, test_size=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Create and train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scheduler(epoch, lr):\n",
        "    if epoch < 10:\n",
        "        return lr\n",
        "    else:\n",
        "        return lr * tf.math.exp(-0.1)\n",
        "\n",
        "callbacks = [\n",
        "    TensorBoard(log_dir=os.path.join(os.path.join('Logs', 'gestures'))),\n",
        "    LearningRateScheduler(scheduler),\n",
        "    EarlyStopping(monitor='val_loss', patience=15, verbose=1),\n",
        "    ModelCheckpoint(\n",
        "        filepath=gestures_weights_path,\n",
        "        save_weights_only=False,\n",
        "        monitor='val_loss',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "gestures_model = Sequential()\n",
        "gestures_model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(X.shape[1], X.shape[2]), recurrent_dropout=0.2))\n",
        "gestures_model.add(BatchNormalization())\n",
        "gestures_model.add(LSTM(128, return_sequences=True, activation='relu', recurrent_dropout=0.2))\n",
        "gestures_model.add(BatchNormalization())\n",
        "gestures_model.add(LSTM(64, return_sequences=False, activation='relu', recurrent_dropout=0.2))\n",
        "gestures_model.add(BatchNormalization())\n",
        "gestures_model.add(Dense(64, activation='relu'))\n",
        "gestures_model.add(BatchNormalization())\n",
        "gestures_model.add(Dense(32, activation='relu'))\n",
        "gestures_model.add(BatchNormalization())\n",
        "gestures_model.add(Dense(y.shape[-1], activation='softmax'))\n",
        "\n",
        "gestures_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gestures_model.fit(X_train, y_train, epochs=50, validation_data=(X_validate, y_validate), callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gestures_model = load_model(gestures_weights_path)\n",
        "\n",
        "run_model = tf.function(lambda x: gestures_model(x))\n",
        "concrete_func = run_model.get_concrete_function(\n",
        "    tf.TensorSpec((None, X.shape[1], X.shape[2]), gestures_model.inputs[0].dtype))\n",
        "\n",
        "gestures_model.summary()\n",
        "gestures_model.save(gestures_weights_path, save_format=\"tf\", signatures=concrete_func)\n",
        "\n",
        "with open('gestures.json', 'w') as fout:\n",
        "    json.dump(classes, fout, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gestures_model = load_model(gestures_weights_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = gestures_model.evaluate(X_test, y_test)\n",
        "print('test loss, test acc:', results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions = gestures_model.predict(X_test)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_classes, predicted_classes)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Convert to TFLite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(gestures_weights_path)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "converter._experimental_lower_tensor_list_ops = False\n",
        "tflite_model = converter.convert()\n",
        "open(converted_gestures_weights_path, 'wb').write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Load converted model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=converted_gestures_weights_path)\n",
        "interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test converted model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "total_loss = 0\n",
        "correct_predictions = 0\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    # Preprocess the input data\n",
        "    test_input = np.expand_dims(X_test[i], axis=0).astype(np.float32)\n",
        "\n",
        "    # Run inference\n",
        "    test_output = tflite_predict(interpreter, test_input)[0]\n",
        "\n",
        "    # Categorical crossentropy loss\n",
        "    total_loss += categorical_crossentropy(y_test[i], test_output)\n",
        "\n",
        "    # Calculate accuracy (for classification tasks)\n",
        "    if np.argmax(test_output) == np.argmax(y_test[i]):\n",
        "        correct_predictions += 1\n",
        "\n",
        "# Compute the final metrics\n",
        "average_loss = total_loss / len(X_test)\n",
        "accuracy = correct_predictions / len(X_test)\n",
        "\n",
        "print(f'Test loss: {average_loss}, Test accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Test live stream"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TimeLandmarks = namedtuple('TimeLandmarks', 'landmarks timestamp')\n",
        "TimePredictions = namedtuple('TimePredictions', 'prediction timestamp')\n",
        "\n",
        "cap = cv2.VideoCapture(cam_idx, api_pref)\n",
        "\n",
        "# Set target resolution\n",
        "cap.set(cv2.CAP_PROP_FRAME_WIDTH, target_width)\n",
        "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, target_height)\n",
        "\n",
        "# Compute dt helpers\n",
        "prev_time = 0\n",
        "cur_time = 0\n",
        "\n",
        "# Timestamp in ms\n",
        "timestamp = 0\n",
        "\n",
        "predicted_gesture = ''\n",
        "prediction_score = 0\n",
        "\n",
        "prev_prediction = None\n",
        "saved_predictions = []\n",
        "\n",
        "# Prev landmarks\n",
        "prev_lh = no_hand_landmarks_ndarray\n",
        "prev_rh = no_hand_landmarks_ndarray\n",
        "prev_la = no_arm_landmarks_ndarray\n",
        "prev_ra = no_arm_landmarks_ndarray\n",
        "\n",
        "# Landmarks one second queue\n",
        "landmarks_queue = []\n",
        "\n",
        "\n",
        "def get_dt():\n",
        "    global prev_time, cur_time\n",
        "\n",
        "    if prev_time == 0:\n",
        "        prev_time = time.time()\n",
        "        return 0\n",
        "\n",
        "    cur_time = time.time()\n",
        "    dt = cur_time - prev_time\n",
        "    prev_time = cur_time\n",
        "\n",
        "    return dt\n",
        "\n",
        "\n",
        "# If no hands for most (70%) of the recorded second, drop the entire recorded second\n",
        "def mostly_empty(seq):\n",
        "    cnt_empty = sum(1 for i in seq if not i[num_hand_features:2*num_hand_features].any())\n",
        "\n",
        "    return cnt_empty / len(seq) > mostly_empty_percentage\n",
        "\n",
        "\n",
        "# Duplicate or sample the recorded second to exactly the number of needed frames, linearly\n",
        "def duplicate_sample(seq, n):\n",
        "    idx = np.round(np.linspace(0, len(seq) - 1, n)).astype(int)\n",
        "    return np.array(seq)[idx]\n",
        "\n",
        "\n",
        "hands_model = vision.HandLandmarker.create_from_options(hands_options)\n",
        "pose_model = vision.PoseLandmarker.create_from_options(pose_options)\n",
        "while cap.isOpened():\n",
        "    # Read frame\n",
        "    _, frame = cap.read()\n",
        "\n",
        "    dt = get_dt()\n",
        "    timestamp += int(dt * 1000)\n",
        "\n",
        "    # Convert to RGB\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    frame.flags.writeable = False\n",
        "    \n",
        "    # Perform detection\n",
        "    hands_results = hands_model.detect_for_video(mp.Image(image_format=mp.ImageFormat.SRGB, data=frame), timestamp)\n",
        "    pose_results = pose_model.detect_for_video(mp.Image(image_format=mp.ImageFormat.SRGB, data=frame), timestamp)\n",
        "\n",
        "    frame.flags.writeable = True\n",
        "\n",
        "    # Get hands landmarks\n",
        "    left_hand_landmarks, right_hand_landmarks = get_hands_landmarks(hands_results, False)\n",
        "    left_hand_world_landmarks, right_hand_world_landmarks = get_hands_landmarks(hands_results, True)\n",
        "\n",
        "    # Get pose landmarks\n",
        "    pose_landmarks = get_pose_landmarks(pose_results, False)\n",
        "    pose_world_landmarks = get_pose_landmarks(pose_results, True)\n",
        "    \n",
        "    # Get frame landmarks\n",
        "    frame_landmarks = get_landmarks_dict(\n",
        "        pose_landmarks,\n",
        "        pose_world_landmarks,\n",
        "        left_hand_landmarks,\n",
        "        left_hand_world_landmarks,\n",
        "        right_hand_landmarks,\n",
        "        right_hand_world_landmarks\n",
        "    )\n",
        "\n",
        "    # Get features from frame landmarks\n",
        "    curr_lh = np.array(frame_landmarks['lh'])\n",
        "    curr_rh = np.array(frame_landmarks['rh'])\n",
        "    curr_la = np.array([frame_landmarks['pose'][11], frame_landmarks['pose'][13], frame_landmarks['pose'][15]])\n",
        "    curr_ra = np.array([frame_landmarks['pose'][12], frame_landmarks['pose'][14], frame_landmarks['pose'][16]])\n",
        "    \n",
        "    rotations = calc_R(skeleton, frame_landmarks['pose'], frame_landmarks['lh'], frame_landmarks['rh'])\n",
        "\n",
        "    features = np.concatenate([\n",
        "        np.array(frame_landmarks['lh_world']).reshape(-1),\n",
        "        np.array(frame_landmarks['rh_world']).reshape(-1),\n",
        "        np.array(np.array([frame_landmarks['pose_world'][11], frame_landmarks['pose_world'][13], frame_landmarks['pose_world'][15]])).reshape(-1),\n",
        "        np.array(np.array([frame_landmarks['pose_world'][12], frame_landmarks['pose_world'][14], frame_landmarks['pose_world'][16]])).reshape(-1),\n",
        "        get_normalized_displacement(prev_lh, curr_lh).reshape(-1),\n",
        "        get_normalized_displacement(prev_rh, curr_rh).reshape(-1),\n",
        "        get_normalized_displacement(prev_la, curr_la).reshape(-1),\n",
        "        get_normalized_displacement(prev_ra, curr_ra).reshape(-1),\n",
        "        rotations[3:22].reshape(-1),\n",
        "        rotations[22:41].reshape(-1)\n",
        "    ])\n",
        "    prev_lh = curr_lh\n",
        "    prev_rh = curr_rh\n",
        "    prev_la = curr_la\n",
        "    prev_ra = curr_ra\n",
        "\n",
        "    # Add new landmarks to queue\n",
        "    landmarks_queue.append(TimeLandmarks(features, timestamp))\n",
        "    \n",
        "    # If we have at least one second\n",
        "    if timestamp - landmarks_queue[0].timestamp >= 1000:\n",
        "        # Remove from queue those older than one second\n",
        "        landmarks_queue = [l for l in landmarks_queue if timestamp - l.timestamp < 1000]\n",
        "\n",
        "        input = [l.landmarks for l in landmarks_queue]\n",
        "\n",
        "        # If no hands were detected for most of the second just drop everything\n",
        "        if mostly_empty(input):\n",
        "            print('empty')\n",
        "            landmarks_queue = []\n",
        "            prediction_score = 0\n",
        "            prev_prediction = None\n",
        "            prev_lh = no_hand_landmarks_ndarray\n",
        "            prev_rh = no_hand_landmarks_ndarray\n",
        "            prev_la = no_arm_landmarks_ndarray\n",
        "            prev_ra = no_arm_landmarks_ndarray\n",
        "        \n",
        "        else:\n",
        "            # Duplicate or sample the frames to the desired fps\n",
        "            input = duplicate_sample(input, inference_fps)\n",
        "\n",
        "            # Make predictions\n",
        "            res = tflite_predict(interpreter, np.expand_dims(input, axis=0).astype(np.float32))[0]\n",
        "            prediction_score = max(res)\n",
        "            predicted_gesture = classes[np.argmax(res)]\n",
        "    \n",
        "    if prediction_score >= prediction_threshold:\n",
        "        # New prediction\n",
        "        if prev_prediction is None or prev_prediction.prediction != predicted_gesture:\n",
        "            prev_prediction = TimePredictions(predicted_gesture, timestamp)\n",
        "\n",
        "        else:\n",
        "            # If the same prediction is consistent for some amount of time, then record it\n",
        "            if timestamp - prev_prediction.timestamp >= min_unchanged_ms:\n",
        "                # Once in this branch, the same prediction would be saved each frame.\n",
        "                # We don't want that, so don't save consecutive duplicates.\n",
        "                # Another approach would be to only record consecutive duplicate predictions once a second.\n",
        "                if not saved_predictions or saved_predictions[-1].prediction != predicted_gesture: #or timestamp - saved_predictions[-1].timestamp >= 1000:\n",
        "                    saved_predictions.append(TimePredictions(predicted_gesture, timestamp))\n",
        "                    print([f'{p.prediction}@{p.timestamp}ms' for p in saved_predictions])\n",
        "\n",
        "    draw_hands_landmarks(frame, left_hand_landmarks, right_hand_landmarks)\n",
        "    draw_pose_landmarks(frame, pose_landmarks)\n",
        "    \n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Mirror frame\n",
        "    frame = cv2.flip(frame, 1)\n",
        "\n",
        "    # Rectangle for FPS and prediction\n",
        "    x, y = 0, 0\n",
        "    w, h = 250, 80\n",
        "    cv2.rectangle(frame, (x, x), (x + w, y + h), (0, 0, 0), -1)\n",
        "    \n",
        "    # Show FPS on screen\n",
        "    if dt:\n",
        "        fps = round(1 / dt)\n",
        "        cv2.putText(frame, str(fps), (5, 35), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 255, 0))\n",
        "\n",
        "    # Show prediction on screen\n",
        "    if saved_predictions:\n",
        "        cv2.putText(\n",
        "            frame,\n",
        "            f'{saved_predictions[-1].prediction}@{saved_predictions[-1].timestamp}ms',\n",
        "            (5, 70),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX,\n",
        "            0.7,\n",
        "            (255, 255, 255),\n",
        "            2\n",
        "            )\n",
        "\n",
        "    # Show frame to screen\n",
        "    cv2.imshow('Camera', frame)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
